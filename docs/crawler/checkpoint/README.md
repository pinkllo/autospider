# Checkpoint å­æ¨¡å—

Checkpoint å­æ¨¡å—å®ç°æ–­ç‚¹ç»­ä¼ ç³»ç»Ÿï¼Œæä¾›è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶å’Œæ¢å¤ç­–ç•¥ï¼Œç¡®ä¿é•¿æ—¶é—´è¿è¡Œçš„çˆ¬å–ä»»åŠ¡èƒ½å¤Ÿä»ä¸­æ–­ç‚¹æ¢å¤ã€‚

---

## ğŸ“ æ¨¡å—ç»“æ„

```
src/autospider/crawler/checkpoint/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ rate_controller.py       # è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶å™¨
â””â”€â”€ resume_strategy.py       # æ–­ç‚¹æ¢å¤ç­–ç•¥
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### âš¡ è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶å™¨ (rate_controller.py)
- `AdaptiveRateController` - é€Ÿç‡æ§åˆ¶å™¨ä¸»ç±»
- `get_delay()` - è·å–å½“å‰å»¶è¿Ÿæ—¶é—´
- `apply_penalty()` - åº”ç”¨æƒ©ç½šï¼ˆé‡åˆ°åçˆ¬æ—¶ï¼‰
- `record_success()` - è®°å½•æˆåŠŸè¯·æ±‚
- `set_level(level)` - è®¾ç½®é™é€Ÿç­‰çº§
- `reset()` - é‡ç½®æ§åˆ¶å™¨çŠ¶æ€

### ğŸ”„ æ–­ç‚¹æ¢å¤ç­–ç•¥ (resume_strategy.py)
- `ResumeStrategy` - æ¢å¤ç­–ç•¥åŸºç±»
- `URLPatternStrategy` - URLè§„å¾‹çˆ†ç ´ç­–ç•¥
- `WidgetJumpStrategy` - æ§ä»¶ç›´è¾¾ç­–ç•¥
- `SmartSkipStrategy` - æ™ºèƒ½è·³è¿‡ç­–ç•¥
- `ResumeCoordinator` - æ¢å¤ç­–ç•¥åè°ƒå™¨

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶

AdaptiveRateController å®ç°æ™ºèƒ½é€Ÿç‡æ§åˆ¶ç®—æ³•ï¼Œæ ¹æ®ç½‘ç«™åçˆ¬æƒ…å†µè‡ªåŠ¨è°ƒæ•´è¯·æ±‚é¢‘ç‡ã€‚

```python
from autospider.crawler.checkpoint.rate_controller import AdaptiveRateController

# åˆ›å»ºé€Ÿç‡æ§åˆ¶å™¨
controller = AdaptiveRateController(
    base_delay=1.0,          # åŸºç¡€å»¶è¿Ÿ1ç§’
    backoff_factor=1.5,      # é€€é¿å› å­1.5
    max_level=5,             # æœ€å¤§é™é€Ÿç­‰çº§5
    credit_recovery_pages=10, # æ¯10ä¸ªæˆåŠŸè¯·æ±‚æ¢å¤ä¸€çº§
    initial_level=0          # åˆå§‹ç­‰çº§0
)

# è·å–å½“å‰å»¶è¿Ÿæ—¶é—´
delay = controller.get_delay()
print(f"å½“å‰å»¶è¿Ÿ: {delay}ç§’")

# æ¨¡æ‹Ÿè¯·æ±‚è¿‡ç¨‹
for page_num in range(1, 21):
    # è·å–å»¶è¿Ÿå¹¶ç­‰å¾…
    current_delay = controller.get_delay()
    print(f"ç¬¬{page_num}é¡µ - å»¶è¿Ÿ: {current_delay:.2f}ç§’")

    # æ¨¡æ‹Ÿè¯·æ±‚ï¼ˆå‡è®¾ç¬¬5é¡µé‡åˆ°åçˆ¬ï¼‰
    if page_num == 5:
        print("é‡åˆ°åçˆ¬æœºåˆ¶ï¼Œåº”ç”¨æƒ©ç½š")
        controller.apply_penalty()
    else:
        # è®°å½•æˆåŠŸè¯·æ±‚
        controller.record_success()

    # ç­‰å¾…å»¶è¿Ÿæ—¶é—´
    await asyncio.sleep(current_delay)

print(f"æœ€ç»ˆé™é€Ÿç­‰çº§: {controller.current_level}")
```

### æ–­ç‚¹æ¢å¤ç­–ç•¥

æä¾›å¤šç§æ¢å¤ç­–ç•¥ï¼Œæ ¹æ®ç½‘ç«™ç‰¹ç‚¹é€‰æ‹©æœ€åˆé€‚çš„æ¢å¤æ–¹å¼ã€‚

```python
from autospider.crawler.checkpoint.resume_strategy import (
    ResumeCoordinator,
    URLPatternStrategy,
    WidgetJumpStrategy,
    SmartSkipStrategy
)

# åˆ›å»ºå„ç§æ¢å¤ç­–ç•¥
url_strategy = URLPatternStrategy(
    list_url="https://example.com/products?page=1"
)

widget_strategy = WidgetJumpStrategy(
    jump_widget_xpath={
        "input": "input.page-input",
        "button": "button.go-btn"
    }
)

smart_strategy = SmartSkipStrategy(
    list_url="https://example.com/products",
    item_xpath="//div[@class='product-item']",
    nav_steps=[
        {"action": "scroll", "direction": "down", "times": 2},
        {"action": "click", "selector": "button.next-page"}
    ]
)

# åˆ›å»ºåè°ƒå™¨
coordinator = ResumeCoordinator([url_strategy, widget_strategy, smart_strategy])

# å°è¯•ä»ç¬¬50é¡µæ¢å¤
current_page = 10
target_page = 50

result = await coordinator.try_resume(current_page, target_page)

if result.success:
    print(f"æˆåŠŸæ¢å¤åˆ°ç¬¬{result.resumed_page}é¡µ")
    print(f"ä½¿ç”¨çš„ç­–ç•¥: {result.strategy_used}")
    print(f"è·³è¿‡çš„é¡µæ•°: {result.pages_skipped}")
else:
    print("æ¢å¤å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†")
    print(f"å¤±è´¥åŸå› : {result.error_message}")
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### é€Ÿç‡æ§åˆ¶ç®—æ³•

åŸºäºæŒ‡æ•°é€€é¿ç­–ç•¥çš„æ™ºèƒ½é€Ÿç‡æ§åˆ¶ï¼š

```python
# å»¶è¿Ÿè®¡ç®—å…¬å¼
current_delay = base_delay * (backoff_factor ^ current_level)

# æƒ©ç½šæœºåˆ¶
- æ¯æ¬¡è§¦å‘æƒ©ç½šï¼Œcurrent_level å¢åŠ 1
- å»¶è¿Ÿæ—¶é—´æŒ‰æŒ‡æ•°å¢é•¿
- é¿å…çŸ­æ—¶é—´å†…é¢‘ç¹è§¦å‘æƒ©ç½š

# æ¢å¤æœºåˆ¶
- æ¯æˆåŠŸå¤„ç† credit_recovery_pages ä¸ªé¡µé¢ï¼Œcurrent_level å‡å°‘1
- é€æ­¥æ¢å¤æ­£å¸¸é€Ÿç‡
- é˜²æ­¢å¿«é€Ÿæ³¢åŠ¨
```

### æ¢å¤ç­–ç•¥é€‰æ‹©

ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¢å¤ç­–ç•¥ï¼š

1. **URLPatternStrategy**ï¼šé€‚ç”¨äºURLåŒ…å«é¡µç å‚æ•°çš„ç½‘ç«™
   - ç›´æ¥æ„é€ ç›®æ ‡é¡µURL
   - å¿«é€Ÿè·³è½¬ï¼Œæ•ˆç‡æœ€é«˜
   - éœ€è¦URLè§„å¾‹æ˜æ˜¾

2. **WidgetJumpStrategy**ï¼šé€‚ç”¨äºä½¿ç”¨é¡µç è¾“å…¥æ§ä»¶çš„ç½‘ç«™
   - æ¨¡æ‹Ÿè¾“å…¥é¡µç å¹¶ç‚¹å‡»ç¡®å®š
   - é€‚ç”¨äºç°ä»£Webåº”ç”¨
   - éœ€è¦æ§ä»¶å®šä½å‡†ç¡®

3. **SmartSkipStrategy**ï¼šå…œåº•æ–¹æ¡ˆï¼Œé€šç”¨æ€§æœ€å¼º
   - ä»ç¬¬ä¸€é¡µå¼€å§‹å¿«é€Ÿæ£€æµ‹
   - æ£€æµ‹åˆ°æ–°æ•°æ®æ—¶å›é€€ä¸€é¡µ
   - ç¡®ä¿æ•°æ®å®Œæ•´æ€§

### çŠ¶æ€æŒä¹…åŒ–

æ”¯æŒæ§åˆ¶å™¨çŠ¶æ€çš„ä¿å­˜å’Œæ¢å¤ï¼š

```python
# ä¿å­˜å½“å‰çŠ¶æ€
state = controller.get_state()
await storage.save_data("rate_controller_state", state)

# æ¢å¤çŠ¶æ€
saved_state = await storage.load_data("rate_controller_state")
if saved_state:
    controller.restore_state(saved_state)
    print("é€Ÿç‡æ§åˆ¶å™¨çŠ¶æ€å·²æ¢å¤")
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„çˆ¬å–ä»»åŠ¡ç®¡ç†

```python
import asyncio
from autospider.crawler.checkpoint.rate_controller import AdaptiveRateController
from autospider.crawler.checkpoint.resume_strategy import ResumeCoordinator
from autospider.common.storage.redis_manager import RedisManager

class CrawlTaskManager:
    """çˆ¬å–ä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self, task_id, list_url, storage_manager):
        self.task_id = task_id
        self.list_url = list_url
        self.storage = storage_manager

        # åˆ›å»ºé€Ÿç‡æ§åˆ¶å™¨
        self.rate_controller = AdaptiveRateController(
            base_delay=1.0,
            backoff_factor=1.5,
            max_level=5,
            credit_recovery_pages=10
        )

        # åˆ›å»ºæ¢å¤ç­–ç•¥åè°ƒå™¨
        self.resume_coordinator = ResumeCoordinator.create_default(list_url)

    async def load_progress(self):
        """åŠ è½½ä»»åŠ¡è¿›åº¦"""
        progress_key = f"task:{self.task_id}:progress"
        progress = await self.storage.get_metadata(progress_key)

        if progress:
            # æ¢å¤é€Ÿç‡æ§åˆ¶å™¨çŠ¶æ€
            if 'rate_controller_state' in progress:
                self.rate_controller.restore_state(progress['rate_controller_state'])

            return progress['current_page'], progress['collected_urls']

        return 1, []  # ä»å¤´å¼€å§‹

    async def save_progress(self, current_page, collected_urls):
        """ä¿å­˜ä»»åŠ¡è¿›åº¦"""
        progress_key = f"task:{self.task_id}:progress"

        progress_data = {
            'current_page': current_page,
            'collected_urls': collected_urls,
            'rate_controller_state': self.rate_controller.get_state(),
            'last_saved': '2026-01-08T10:00:00Z'
        }

        await self.storage.save_item(progress_key, progress_data)

    async def crawl_page(self, page_num):
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        # è·å–å½“å‰å»¶è¿Ÿ
        delay = self.rate_controller.get_delay()
        print(f"çˆ¬å–ç¬¬{page_num}é¡µï¼Œå»¶è¿Ÿ: {delay:.2f}ç§’")

        # ç­‰å¾…å»¶è¿Ÿæ—¶é—´
        await asyncio.sleep(delay)

        try:
            # æ¨¡æ‹Ÿé¡µé¢çˆ¬å–ï¼ˆè¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„çˆ¬å–é€»è¾‘ï¼‰
            if page_num % 7 == 0:  # æ¨¡æ‹Ÿå¶å°”é‡åˆ°åçˆ¬
                print(f"ç¬¬{page_num}é¡µé‡åˆ°åçˆ¬æœºåˆ¶")
                self.rate_controller.apply_penalty()
                raise Exception("Anti-crawler detected")

            # æ¨¡æ‹ŸæˆåŠŸçˆ¬å–
            urls = [f"https://example.com/product/{page_num * 10 + i}"
                   for i in range(10)]

            self.rate_controller.record_success()
            return urls

        except Exception as e:
            print(f"çˆ¬å–ç¬¬{page_num}é¡µå¤±è´¥: {e}")
            return []

    async def resume_to_page(self, current_page, target_page):
        """æ¢å¤åˆ°æŒ‡å®šé¡µé¢"""
        print(f"å°è¯•ä»ç¬¬{current_page}é¡µæ¢å¤åˆ°ç¬¬{target_page}é¡µ")

        result = await self.resume_coordinator.try_resume(current_page, target_page)

        if result.success:
            print(f"æˆåŠŸæ¢å¤åˆ°ç¬¬{result.resumed_page}é¡µ")
            return result.resumed_page
        else:
            print("æ¢å¤å¤±è´¥ï¼Œä»å¤´å¼€å§‹")
            return 1

    async def run(self, max_pages=100):
        """è¿è¡Œçˆ¬å–ä»»åŠ¡"""
        # åŠ è½½è¿›åº¦
        current_page, collected_urls = await self.load_progress()

        print(f"å¼€å§‹çˆ¬å–ï¼Œå½“å‰è¿›åº¦: ç¬¬{current_page}é¡µ")

        while current_page <= max_pages:
            # çˆ¬å–å½“å‰é¡µé¢
            new_urls = await self.crawl_page(current_page)
            collected_urls.extend(new_urls)

            # æ¯5é¡µä¿å­˜ä¸€æ¬¡è¿›åº¦
            if current_page % 5 == 0:
                await self.save_progress(current_page, collected_urls)
                print(f"è¿›åº¦å·²ä¿å­˜: ç¬¬{current_page}é¡µï¼ŒURLæ•°é‡: {len(collected_urls)}")

            current_page += 1

        # ä»»åŠ¡å®Œæˆï¼Œæ¸…ç†è¿›åº¦æ•°æ®
        await self.storage.delete(f"task:{self.task_id}:progress")
        print(f"ä»»åŠ¡å®Œæˆ! å…±æ”¶é›† {len(collected_urls)} ä¸ªURL")

        return collected_urls

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åˆ›å»ºå­˜å‚¨ç®¡ç†å™¨
    storage = RedisManager(key_prefix="crawler:")
    await storage.connect()

    try:
        # åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨
        task_manager = CrawlTaskManager(
            task_id="demo_task",
            list_url="https://example.com/products?page=1",
            storage_manager=storage
        )

        # è¿è¡Œçˆ¬å–ä»»åŠ¡
        urls = await task_manager.run(max_pages=20)

        print(f"æœ€ç»ˆæ”¶é›†çš„URLæ•°é‡: {len(urls)}")

    finally:
        await storage.disconnect()

asyncio.run(main())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### é€Ÿç‡æ§åˆ¶é…ç½®

1. **åŸºç¡€å»¶è¿Ÿ**ï¼šæ ¹æ®ç›®æ ‡ç½‘ç«™å“åº”æ—¶é—´è®¾ç½®
2. **é€€é¿å› å­**ï¼š1.3-2.0ä¹‹é—´ï¼Œé¿å…è¿‡äºæ¿€è¿›
3. **æœ€å¤§ç­‰çº§**ï¼š3-5çº§ï¼Œé¿å…å»¶è¿Ÿè¿‡é•¿
4. **æ¢å¤é˜ˆå€¼**ï¼š8-15ä¸ªæˆåŠŸè¯·æ±‚æ¢å¤ä¸€çº§

### æ¢å¤ç­–ç•¥é€‰æ‹©

1. **ä¼˜å…ˆURLæ¨¡å¼**ï¼šURLè§„å¾‹æ˜æ˜¾æ—¶æ•ˆç‡æœ€é«˜
2. **æ¬¡é€‰æ§ä»¶è·³è½¬**ï¼šç°ä»£Webåº”ç”¨é€‚ç”¨
3. **å…œåº•æ™ºèƒ½è·³è¿‡**ï¼šé€šç”¨æ€§å¼ºä½†æ•ˆç‡è¾ƒä½
4. **æ··åˆç­–ç•¥**ï¼šæ ¹æ®å®é™…æƒ…å†µç»„åˆä½¿ç”¨

### çŠ¶æ€ç®¡ç†

1. **å®šæœŸä¿å­˜**ï¼šæ¯çˆ¬å–ä¸€å®šæ•°é‡é¡µé¢åä¿å­˜çŠ¶æ€
2. **å¼‚å¸¸å¤„ç†**ï¼šæ•è·å¼‚å¸¸å¹¶ä¿å­˜å½“å‰çŠ¶æ€
3. **çŠ¶æ€éªŒè¯**ï¼šæ¢å¤æ—¶éªŒè¯çŠ¶æ€å®Œæ•´æ€§
4. **æ¸…ç†æœºåˆ¶**ï¼šä»»åŠ¡å®Œæˆåæ¸…ç†çŠ¶æ€æ•°æ®

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é€Ÿç‡æ§åˆ¶è¿‡äºä¿å®ˆ**
   - è°ƒæ•´åŸºç¡€å»¶è¿Ÿå’Œé€€é¿å› å­
   - å¢åŠ æ¢å¤é˜ˆå€¼
   - è€ƒè™‘ç½‘ç«™çš„å®é™…åçˆ¬å¼ºåº¦

2. **æ¢å¤ç­–ç•¥å¤±è´¥**
   - æ£€æŸ¥URLæ¨¡å¼æ˜¯å¦æ­£ç¡®
   - éªŒè¯æ§ä»¶å®šä½å‡†ç¡®æ€§
   - è€ƒè™‘ä½¿ç”¨å…œåº•ç­–ç•¥

3. **çŠ¶æ€æ¢å¤å¼‚å¸¸**
   - æ£€æŸ¥çŠ¶æ€æ•°æ®å®Œæ•´æ€§
   - éªŒè¯åºåˆ—åŒ–/ååºåˆ—åŒ–è¿‡ç¨‹
   - ç¡®è®¤å­˜å‚¨åç«¯æ­£å¸¸å·¥ä½œ

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# ç›‘æ§é€Ÿç‡æ§åˆ¶å™¨çŠ¶æ€
print(f"å½“å‰ç­‰çº§: {controller.current_level}")
print(f"æˆåŠŸè®¡æ•°: {controller.success_count}")
print(f"ä¿¡ç”¨å€¼: {controller.credit}")

# æµ‹è¯•æ¢å¤ç­–ç•¥
for strategy in coordinator.strategies:
    print(f"ç­–ç•¥ {strategy.__class__.__name__}: {strategy.description}")

# æ€§èƒ½ç›‘æ§
import time
start_time = time.time()
# æ‰§è¡Œæ“ä½œ
end_time = time.time()
print(f"æ“ä½œè€—æ—¶: {end_time - start_time:.3f}ç§’")
```

---

*æœ€åæ›´æ–°: 2026-01-08*

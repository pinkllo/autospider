# batch_collector.py - æ‰¹é‡çˆ¬å–å™¨

batch_collector.py æ¨¡å—æä¾›åŸºäºé…ç½®æ–‡ä»¶æ‰§è¡Œæ‰¹é‡ URL æ”¶é›†çš„åŠŸèƒ½ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬ã€‚

---

## ğŸ“ æ–‡ä»¶è·¯å¾„

```
src/autospider/crawler/batch_collector.py
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸš€ æ ¸å¿ƒç±»
- `BatchCollector` - æ‰¹é‡çˆ¬å–å™¨ä¸»ç±»

### ğŸ”§ ä¸»è¦æ–¹æ³•
- `run()` - è¿è¡Œæ”¶é›†æµç¨‹
- `collect_from_config()` - ä»é…ç½®æ–‡ä»¶æ‰§è¡Œæ‰¹é‡æ”¶é›†
- `_load_config()` - åŠ è½½é…ç½®æ–‡ä»¶
- `_initialize_handlers()` - åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨

### ğŸ” å†…éƒ¨æ–¹æ³•
- `_preload_config()` - é¢„åŠ è½½é…ç½®æ–‡ä»¶
- `_resume_to_target_page()` - ä½¿ç”¨ä¸‰é˜¶æ®µç­–ç•¥æ¢å¤åˆ°ç›®æ ‡é¡µ
- `_save_progress()` - ä¿å­˜æ”¶é›†è¿›åº¦
- `_create_result()` - åˆ›å»ºæ”¶é›†ç»“æœ
- `_create_empty_result()` - åˆ›å»ºç©ºç»“æœ

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### BatchCollector

æ‰¹é‡çˆ¬å–å™¨ï¼Œç»§æ‰¿è‡ª BaseCollectorï¼ŒåŸºäºé…ç½®æ–‡ä»¶æ‰§è¡Œæ‰¹é‡ URL æ”¶é›†ã€‚

```python
from autospider.crawler.batch_collector import BatchCollector

# åˆ›å»ºæ‰¹é‡çˆ¬å–å™¨
collector = BatchCollector(
    page=page,
    config_path="output/collection_config.json",
    output_dir="output"
)

# è¿è¡Œæ”¶é›†æµç¨‹
result = await collector.run()

print(f"æ”¶é›†åˆ° {len(result.collected_urls)} ä¸ª URL")
```

### æ”¶é›†æµç¨‹

BatchCollector å®ç°åŸºäºé…ç½®æ–‡ä»¶çš„æ”¶é›†æµç¨‹ï¼š

**Phase 0: åŠ è½½é…ç½®**
```python
# åŠ è½½é…ç½®æ–‡ä»¶
collection_config = CollectionConfig.from_dict(data)

# æå–é…ç½®ä¿¡æ¯
list_url = collection_config.list_url
task_description = collection_config.task_description
nav_steps = collection_config.nav_steps
common_detail_xpath = collection_config.common_detail_xpath
```

**Phase 1: å¯¼èˆªåˆ°åˆ—è¡¨é¡µ**
```python
# å¯¼èˆªåˆ°åˆ—è¡¨é¡µ
await page.goto(list_url, wait_until="domcontentloaded", timeout=30000)
```

**Phase 2: é‡æ”¾å¯¼èˆªæ­¥éª¤**
```python
# é‡æ”¾å·²ä¿å­˜çš„å¯¼èˆªæ­¥éª¤
nav_success = await navigation_handler.replay_nav_steps(nav_steps)
```

**Phase 3: æ–­ç‚¹æ¢å¤**
```python
# è·³è½¬åˆ°ç›®æ ‡é¡µ
actual_page = await collector._resume_to_target_page(target_page_num)
```

**Phase 4: æ”¶é›†é˜¶æ®µ**
```python
# ä½¿ç”¨å…¬å…± xpath éå†åˆ—è¡¨é¡µ
await collector._collect_phase_with_xpath()

# æˆ–ä½¿ç”¨ LLM éå†
await collector._collect_phase_with_llm()
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### é…ç½®æ–‡ä»¶é©±åŠ¨

BatchCollector ä»é…ç½®æ–‡ä»¶è¯»å–æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼š

```python
# é…ç½®æ–‡ä»¶ç»“æ„
{
    "list_url": "https://example.com/list",
    "task_description": "æ”¶é›†å•†å“è¯¦æƒ…é¡µé“¾æ¥",
    "nav_steps": [...],
    "common_detail_xpath": "//a[@class='product-link']",
    "pagination_xpath": "//a[contains(text(),'ä¸‹ä¸€é¡µ')]",
    "jump_widget_xpath": {
        "input": "//input[@class='page-input']",
        "button": "//button[@class='jump-btn']"
    }
}
```

### æ–­ç‚¹ç»­çˆ¬

æ”¯æŒä»ä¸Šæ¬¡ä¸­æ–­çš„ä½ç½®ç»§ç»­æ”¶é›†ï¼š

```python
# åŠ è½½å†å²è¿›åº¦
previous_progress = progress_persistence.load_progress()

# æ¢å¤é€Ÿç‡æ§åˆ¶å™¨çŠ¶æ€
rate_controller.current_level = previous_progress.backoff_level
rate_controller.consecutive_success_count = previous_progress.consecutive_success_pages

# è·³è½¬åˆ°ç›®æ ‡é¡µ
actual_page = await collector._resume_to_target_page(target_page_num)
```

### ä¸¤ç§æ”¶é›†æ¨¡å¼

1. **XPath æ¨¡å¼**ï¼šä½¿ç”¨å…¬å…± XPath ç›´æ¥æå– URLï¼ˆå¿«é€Ÿã€ç¨³å®šï¼‰
2. **LLM æ¨¡å¼**ï¼šä½¿ç”¨ LLM è¯†åˆ«è¯¦æƒ…é¡µé“¾æ¥ï¼ˆçµæ´»ã€æ™ºèƒ½ï¼‰

```python
if common_detail_xpath:
    # XPath æ¨¡å¼
    await collector._collect_phase_with_xpath()
else:
    # LLM æ¨¡å¼
    await collector._collect_phase_with_llm()
```

### é…ç½®æŒä¹…åŒ–

è‡ªåŠ¨ä¿å­˜é…ç½®å’Œè¿›åº¦ï¼š

```python
# ä¿å­˜é…ç½®
collection_config = CollectionConfig(
    nav_steps=nav_steps,
    common_detail_xpath=common_detail_xpath,
    pagination_xpath=pagination_xpath,
    jump_widget_xpath=jump_widget_xpath,
    list_url=list_url,
    task_description=task_description,
)
config_persistence.save(collection_config)

# ä¿å­˜è¿›åº¦
progress = CollectionProgress(
    status="RUNNING",
    list_url=list_url,
    task_description=task_description,
    current_page_num=current_page_num,
    collected_count=len(collected_urls),
    backoff_level=rate_controller.current_level,
    consecutive_success_pages=rate_controller.consecutive_success_count,
)
progress_persistence.save_progress(progress)
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„æ‰¹é‡æ”¶é›†æµç¨‹

```python
import asyncio
from playwright.async_api import async_playwright
from autospider.crawler.batch_collector import BatchCollector

async def batch_collect():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        # åˆ›å»ºæ‰¹é‡çˆ¬å–å™¨
        collector = BatchCollector(
            page=page,
            config_path="output/collection_config.json",
            output_dir="output"
        )

        # è¿è¡Œæ”¶é›†æµç¨‹
        result = await collector.run()

        print(f"æ”¶é›†åˆ° {len(result.collected_urls)} ä¸ª URL")

        await browser.close()

# è¿è¡Œ
asyncio.run(batch_collect())
```

### ä½¿ç”¨ä¾¿æ·å‡½æ•°

```python
from autospider.crawler.batch_collector import batch_collect_urls

# ä½¿ç”¨ä¾¿æ·å‡½æ•°
result = await batch_collect_urls(
    page=page,
    config_path="output/collection_config.json",
    output_dir="output"
)

print(f"æ”¶é›†åˆ° {len(result.collected_urls)} ä¸ª URL")
```

### æ–­ç‚¹ç»­çˆ¬

```python
# æ”¶é›†å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤ä¹‹å‰çš„è¿›åº¦
collector = BatchCollector(
    page=page,
    config_path="output/collection_config.json",
    output_dir="output"
)

# å¦‚æœä¹‹å‰ä¸­æ–­è¿‡ï¼Œä¼šè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­
result = await collector.run()
```

### è‡ªå®šä¹‰è¾“å‡ºç›®å½•

```python
# æŒ‡å®šä¸åŒçš„è¾“å‡ºç›®å½•
collector = BatchCollector(
    page=page,
    config_path="configs/my_config.json",
    output_dir="output/my_collection"
)

result = await collector.run()
```

---

## ğŸ“ æœ€ä½³å®è·µ

### é…ç½®æ–‡ä»¶ç®¡ç†

1. **ç‰ˆæœ¬æ§åˆ¶**ï¼šå°†é…ç½®æ–‡ä»¶çº³å…¥ç‰ˆæœ¬æ§åˆ¶
2. **å‘½åè§„èŒƒ**ï¼šä½¿ç”¨æœ‰æ„ä¹‰çš„é…ç½®æ–‡ä»¶å
3. **æ–‡æ¡£è¯´æ˜**ï¼šä¸ºé…ç½®æ–‡ä»¶æ·»åŠ æ³¨é‡Šè¯´æ˜

### æ–­ç‚¹ç»­çˆ¬

1. **å®šæœŸä¿å­˜**ï¼šæ¯é¡µæ”¶é›†åä¿å­˜è¿›åº¦
2. **éªŒè¯é…ç½®**ï¼šç¡®ä¿å†å²é…ç½®ä¸å½“å‰ä»»åŠ¡åŒ¹é…
3. **æ¢å¤çŠ¶æ€**ï¼šæ¢å¤é€Ÿç‡æ§åˆ¶å™¨ç­‰çŠ¶æ€

### æ”¶é›†æ¨¡å¼é€‰æ‹©

1. **ä¼˜å…ˆ XPath**ï¼šå¦‚æœå·²æå–å…¬å…± XPathï¼Œä¼˜å…ˆä½¿ç”¨ XPath æ¨¡å¼
2. **LLM å¤‡ç”¨**ï¼šå¦‚æœ XPath ä¸å¯ç”¨ï¼Œä½¿ç”¨ LLM æ¨¡å¼
3. **æ€§èƒ½è€ƒè™‘**ï¼šXPath æ¨¡å¼æ¯” LLM æ¨¡å¼æ›´å¿«é€Ÿã€æ›´ç¨³å®š

### é”™è¯¯å¤„ç†

1. **æ•è·å¼‚å¸¸**ï¼šå¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
2. **åº”ç”¨æƒ©ç½š**ï¼šé­é‡åçˆ¬æ—¶åº”ç”¨é€Ÿç‡æƒ©ç½š
3. **è®°å½•æ—¥å¿—**ï¼šè¯¦ç»†è®°å½•æ“ä½œæ—¥å¿—ä¾¿äºè°ƒè¯•

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥**
   - æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
   - éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨

2. **å¯¼èˆªæ­¥éª¤é‡æ”¾å¤±è´¥**
   - æ£€æŸ¥å¯¼èˆªæ­¥éª¤æ˜¯å¦æ­£ç¡®
   - éªŒè¯é¡µé¢ç»“æ„æ˜¯å¦å‘ç”Ÿå˜åŒ–
   - ç¡®è®¤å…ƒç´ é€‰æ‹©å™¨æ˜¯å¦æœ‰æ•ˆ

3. **æ”¶é›†é˜¶æ®µå¡ä½**
   - æ£€æŸ¥åˆ†é¡µæ§ä»¶æ˜¯å¦æ­£ç¡®è¯†åˆ«
   - éªŒè¯é€Ÿç‡æ§åˆ¶å»¶è¿Ÿæ˜¯å¦åˆç†
   - ç¡®è®¤ç›®æ ‡ URL æ•°é‡æ˜¯å¦å¯è¾¾æˆ

4. **æ–­ç‚¹æ¢å¤å¤±è´¥**
   - æ£€æŸ¥å†å²é…ç½®æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…
   - éªŒè¯è¿›åº¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
   - ç¡®è®¤è·³è½¬æ§ä»¶ XPath æ˜¯å¦æ­£ç¡®

### è°ƒè¯•æŠ€å·§

```python
# æ£€æŸ¥é…ç½®åŠ è½½
print(f"åˆ—è¡¨é¡µ: {collector.list_url}")
print(f"ä»»åŠ¡æè¿°: {collector.task_description}")
print(f"å¯¼èˆªæ­¥éª¤: {len(collector.nav_steps)}")
print(f"å…¬å…± XPath: {collector.common_detail_xpath}")

# æ£€æŸ¥æ”¶é›†è¿›åº¦
print(f"å½“å‰é¡µ: {pagination_handler.current_page_num}")
print(f"å·²æ”¶é›†: {len(collector.collected_urls)}")
print(f"é™é€Ÿç­‰çº§: {rate_controller.current_level}")

# æ£€æŸ¥é…ç½®æ–‡ä»¶
import json
config_data = json.loads(Path("output/collection_config.json").read_text())
print(json.dumps(config_data, indent=2))
```

---

## ğŸ“š æ–¹æ³•å‚è€ƒ

### BatchCollector æ–¹æ³•

| æ–¹æ³• | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `run()` | æ—  | URLCollectorResult | è¿è¡Œæ”¶é›†æµç¨‹ |
| `collect_from_config()` | æ—  | URLCollectorResult | ä»é…ç½®æ–‡ä»¶æ‰§è¡Œæ‰¹é‡æ”¶é›† |
| `_load_config()` | æ—  | bool | åŠ è½½é…ç½®æ–‡ä»¶ |
| `_initialize_handlers()` | æ—  | None | åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨ |
| `_preload_config()` | æ—  | None | é¢„åŠ è½½é…ç½®æ–‡ä»¶ |
| `_resume_to_target_page()` | target_page_num, jump_widget_xpath, pagination_xpath | int | ä½¿ç”¨ä¸‰é˜¶æ®µç­–ç•¥æ¢å¤åˆ°ç›®æ ‡é¡µ |
| `_save_progress()` | æ—  | None | ä¿å­˜æ”¶é›†è¿›åº¦ |
| `_create_result()` | æ—  | URLCollectorResult | åˆ›å»ºæ”¶é›†ç»“æœ |
| `_create_empty_result()` | æ—  | URLCollectorResult | åˆ›å»ºç©ºç»“æœ |

### ä¾¿æ·å‡½æ•°

| å‡½æ•° | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `batch_collect_urls()` | page, config_path, output_dir | URLCollectorResult | æ‰¹é‡æ”¶é›† URL çš„ä¾¿æ·å‡½æ•° |

---

## ğŸ“„ é…ç½®æ–‡ä»¶æ ¼å¼

### collection_config.json

```json
{
    "list_url": "https://example.com/list",
    "task_description": "æ”¶é›†å•†å“è¯¦æƒ…é¡µé“¾æ¥",
    "nav_steps": [
        {
            "action": "click",
            "mark_id": 5,
            "target_text": "ç­›é€‰æŒ‰é’®"
        }
    ],
    "common_detail_xpath": "//a[@class='product-link']",
    "pagination_xpath": "//a[contains(text(),'ä¸‹ä¸€é¡µ')]",
    "jump_widget_xpath": {
        "input": "//input[@class='page-input']",
        "button": "//button[@class='jump-btn']"
    }
}
```

### é…ç½®å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `list_url` | string | æ˜¯ | åˆ—è¡¨é¡µ URL |
| `task_description` | string | æ˜¯ | ä»»åŠ¡æè¿° |
| `nav_steps` | array | å¦ | å¯¼èˆªæ­¥éª¤åˆ—è¡¨ |
| `common_detail_xpath` | string | å¦ | å…¬å…±è¯¦æƒ…é¡µ XPath |
| `pagination_xpath` | string | å¦ | åˆ†é¡µæ§ä»¶ XPath |
| `jump_widget_xpath` | object | å¦ | è·³è½¬æ§ä»¶ XPath |

---

*æœ€åæ›´æ–°: 2026-01-08*

# base_collector.py - æ”¶é›†å™¨åŸºç±»

base_collector.py æ¨¡å—æä¾› URL æ”¶é›†å™¨çš„åŸºç±»ï¼ŒæŠ½å– URLCollector å’Œ BatchCollector çš„å…¬å…±é€»è¾‘ï¼Œå‡å°‘ä»£ç é‡å¤ï¼Œæé«˜å¯ç»´æŠ¤æ€§ã€‚

---

## ğŸ“ æ–‡ä»¶è·¯å¾„

```
src/autospider/crawler/base_collector.py
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸš€ æ ¸å¿ƒç±»
- `BaseCollector` - URL æ”¶é›†å™¨åŸºç±»ï¼ˆæŠ½è±¡ç±»ï¼‰

### ğŸ”§ ä¸»è¦æ–¹æ³•
- `run()` - è¿è¡Œæ”¶é›†æµç¨‹ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
- `_initialize_handlers()` - åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨
- `_load_previous_urls()` - ä» Redis åŠ è½½å†å² URL
- `_resume_to_target_page()` - ä½¿ç”¨ä¸‰é˜¶æ®µç­–ç•¥æ¢å¤åˆ°ç›®æ ‡é¡µ
- `_collect_phase_with_xpath()` - æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨å…¬å…± XPath
- `_collect_phase_with_llm()` - æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨ LLM
- `_save_progress()` - ä¿å­˜æ”¶é›†è¿›åº¦

### ğŸ” å†…éƒ¨æ–¹æ³•
- `_is_progress_compatible()` - æ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…
- `_extract_urls_with_xpath()` - ä½¿ç”¨ XPath æå–å½“å‰é¡µçš„ URL
- `_collect_page_with_llm()` - ä½¿ç”¨ LLM æ”¶é›†å•é¡µçš„ URL
- `_create_result()` - åˆ›å»ºæ”¶é›†ç»“æœ

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### BaseCollector

URL æ”¶é›†å™¨åŸºç±»ï¼Œæä¾›å…¬å…±çš„æ”¶é›†é€»è¾‘ã€‚

```python
from autospider.crawler.base_collector import BaseCollector

# BaseCollector æ˜¯æŠ½è±¡ç±»ï¼Œéœ€è¦å­ç±»å®ç°
class MyCollector(BaseCollector):
    async def run(self) -> URLCollectorResult:
        # å®ç°æ”¶é›†æµç¨‹
        return self._create_result()

# åˆ›å»ºæ”¶é›†å™¨
collector = MyCollector(
    page=page,
    list_url="https://example.com/list",
    task_description="æ”¶é›†è¯¦æƒ…é¡µé“¾æ¥",
    output_dir="output"
)

# è¿è¡Œæ”¶é›†æµç¨‹
result = await collector.run()
```

### å…¬å…±åŠŸèƒ½

BaseCollector æä¾›ä»¥ä¸‹å…¬å…±åŠŸèƒ½ï¼š

1. **é€Ÿç‡æ§åˆ¶**
```python
# é€Ÿç‡æ§åˆ¶å™¨è‡ªåŠ¨åˆå§‹åŒ–
self.rate_controller = AdaptiveRateController()

# è·å–å½“å‰å»¶è¿Ÿ
delay = self.rate_controller.get_delay()

# è®°å½•æˆåŠŸ
self.rate_controller.record_success()

# åº”ç”¨æƒ©ç½š
self.rate_controller.apply_penalty()
```

2. **æ–­ç‚¹ç»­çˆ¬**
```python
# åŠ è½½å†å² URL
await self._load_previous_urls()

# æ¢å¤åˆ°ç›®æ ‡é¡µ
actual_page = await self._resume_to_target_page(target_page_num)

# ä¿å­˜è¿›åº¦
self._save_progress()
```

3. **Redis æŒä¹…åŒ–**
```python
# Redis ç®¡ç†å™¨è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
self.redis_manager: RedisManager | None = None

# ä¿å­˜ URL
if self.redis_manager:
    await self.redis_manager.save_item(url)

# åŠ è½½ URL
if self.redis_manager:
    urls = await self.redis_manager.load_items()
```

4. **XPath/LLM æ”¶é›†**
```python
# ä½¿ç”¨ XPath æ”¶é›†
await self._collect_phase_with_xpath()

# ä½¿ç”¨ LLM æ”¶é›†
await self._collect_phase_with_llm()
```

5. **åˆ†é¡µå¤„ç†**
```python
# åˆ†é¡µå¤„ç†å™¨è‡ªåŠ¨åˆå§‹åŒ–
self.pagination_handler = PaginationHandler(...)

# ç‚¹å‡»ä¸‹ä¸€é¡µ
page_turned = await self.pagination_handler.find_and_click_next_page()
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### æŠ½è±¡åŸºç±»è®¾è®¡

BaseCollector ä½¿ç”¨æŠ½è±¡åŸºç±»è®¾è®¡ï¼Œå¼ºåˆ¶å­ç±»å®ç° `run()` æ–¹æ³•ï¼š

```python
from abc import ABC, abstractmethod

class BaseCollector(ABC):
    @abstractmethod
    async def run(self) -> URLCollectorResult:
        """è¿è¡Œæ”¶é›†æµç¨‹ï¼ˆå­ç±»å®ç°ï¼‰"""
        pass
```

### å¤„ç†å™¨å»¶è¿Ÿåˆå§‹åŒ–

å¤„ç†å™¨åœ¨ `_initialize_handlers()` æ–¹æ³•ä¸­å»¶è¿Ÿåˆå§‹åŒ–ï¼š

```python
def _initialize_handlers(self) -> None:
    """åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨
    
    å­ç±»å¯è¦†ç›–æ­¤æ–¹æ³•æ·»åŠ é¢å¤–çš„åˆå§‹åŒ–é€»è¾‘ã€‚
    """
    self.url_extractor = URLExtractor(self.page, self.list_url)
    self.navigation_handler = NavigationHandler(...)
    self.pagination_handler = PaginationHandler(...)
```

### é…ç½®é©±åŠ¨çš„ Redis

Redis ç®¡ç†å™¨æ ¹æ®é…ç½®è‡ªåŠ¨åˆå§‹åŒ–ï¼š

```python
def _init_redis_manager(self) -> None:
    """åˆå§‹åŒ– Redis ç®¡ç†å™¨"""
    if not config.redis.enabled:
        return
    
    try:
        from ..common.storage.redis_manager import RedisManager
        self.redis_manager = RedisManager(...)
    except ImportError:
        logger.warning("Redis ä¾èµ–æœªå®‰è£…")
```

### è¿›åº¦å…¼å®¹æ€§æ£€æŸ¥

åœ¨æ¢å¤è¿›åº¦å‰æ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…ï¼š

```python
def _is_progress_compatible(self, progress: CollectionProgress | None) -> bool:
    """æ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…"""
    if not progress:
        return False
    if progress.list_url and progress.list_url != self.list_url:
        return False
    if progress.task_description and progress.task_description != self.task_description:
        return False
    return True
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºè‡ªå®šä¹‰æ”¶é›†å™¨

```python
from autospider.crawler.base_collector import BaseCollector
from autospider.extractor.collector import URLCollectorResult

class CustomCollector(BaseCollector):
    """è‡ªå®šä¹‰æ”¶é›†å™¨"""
    
    def __init__(self, page, list_url, task_description, output_dir="output"):
        super().__init__(page, list_url, task_description, output_dir)
        
        # æ·»åŠ è‡ªå®šä¹‰åˆå§‹åŒ–
        self.custom_config = {}
    
    def _initialize_handlers(self) -> None:
        """è¦†ç›–åˆå§‹åŒ–æ–¹æ³•"""
        super()._initialize_handlers()
        
        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†å™¨
        pass
    
    async def run(self) -> URLCollectorResult:
        """å®ç°æ”¶é›†æµç¨‹"""
        # 1. åŠ è½½å†å² URL
        await self._load_previous_urls()
        
        # 2. å¯¼èˆªåˆ°åˆ—è¡¨é¡µ
        await self.page.goto(self.list_url, wait_until="domcontentloaded")
        
        # 3. åˆå§‹åŒ–å¤„ç†å™¨
        self._initialize_handlers()
        
        # 4. æ”¶é›† URL
        if self.common_detail_xpath:
            await self._collect_phase_with_xpath()
        else:
            await self._collect_phase_with_llm()
        
        # 5. è¿”å›ç»“æœ
        return self._create_result()

# ä½¿ç”¨è‡ªå®šä¹‰æ”¶é›†å™¨
collector = CustomCollector(
    page=page,
    list_url="https://example.com/list",
    task_description="æ”¶é›†è¯¦æƒ…é¡µé“¾æ¥",
    output_dir="output"
)

result = await collector.run()
```

### è¦†ç›–åˆå§‹åŒ–æ–¹æ³•

```python
class EnhancedCollector(BaseCollector):
    """å¢å¼ºå‹æ”¶é›†å™¨"""
    
    def _initialize_handlers(self) -> None:
        """è¦†ç›–åˆå§‹åŒ–æ–¹æ³•ï¼Œæ·»åŠ é¢å¤–é€»è¾‘"""
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super()._initialize_handlers()
        
        # æ·»åŠ è‡ªå®šä¹‰åˆå§‹åŒ–
        self.custom_handler = CustomHandler(...)
        
        # é…ç½®å¤„ç†å™¨
        self.pagination_handler.custom_config = {...}
```

### è‡ªå®šä¹‰è¿›åº¦ä¿å­˜

```python
class CustomCollector(BaseCollector):
    """è‡ªå®šä¹‰è¿›åº¦ä¿å­˜"""
    
    def _save_progress(self) -> None:
        """è¦†ç›–è¿›åº¦ä¿å­˜æ–¹æ³•"""
        # è°ƒç”¨çˆ¶ç±»ä¿å­˜
        super()._save_progress()
        
        # æ·»åŠ è‡ªå®šä¹‰ä¿å­˜é€»è¾‘
        custom_progress = {
            "custom_field": self.custom_value,
            "timestamp": datetime.now().isoformat()
        }
        
        custom_file = self.output_dir / "custom_progress.json"
        custom_file.write_text(json.dumps(custom_progress))
```

---

## ğŸ“ æœ€ä½³å®è·µ

### ç»§æ‰¿ BaseCollector

1. **å®ç°æŠ½è±¡æ–¹æ³•**ï¼šå¿…é¡»å®ç° `run()` æ–¹æ³•
2. **è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–**ï¼šåœ¨ `__init__` ä¸­è°ƒç”¨ `super().__init__()`
3. **è¦†ç›–åˆå§‹åŒ–æ–¹æ³•**ï¼šåœ¨ `_initialize_handlers()` ä¸­æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
4. **ä½¿ç”¨å…¬å…±åŠŸèƒ½**ï¼šå……åˆ†åˆ©ç”¨åŸºç±»æä¾›çš„é€Ÿç‡æ§åˆ¶ã€æ–­ç‚¹ç»­çˆ¬ç­‰åŠŸèƒ½

### å¤„ç†å™¨ç®¡ç†

1. **å»¶è¿Ÿåˆå§‹åŒ–**ï¼šåœ¨ `_initialize_handlers()` ä¸­åˆå§‹åŒ–å¤„ç†å™¨
2. **æ£€æŸ¥ None**ï¼šä½¿ç”¨å¤„ç†å™¨å‰æ£€æŸ¥æ˜¯å¦ä¸º None
3. **é…ç½®å¤„ç†å™¨**ï¼šæ ¹æ®éœ€è¦é…ç½®å¤„ç†å™¨çš„å‚æ•°

### è¿›åº¦ç®¡ç†

1. **å®šæœŸä¿å­˜**ï¼šæ¯é¡µæ”¶é›†åä¿å­˜è¿›åº¦
2. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šæ¢å¤è¿›åº¦å‰æ£€æŸ¥å…¼å®¹æ€§
3. **çŠ¶æ€æ¢å¤**ï¼šæ¢å¤é€Ÿç‡æ§åˆ¶å™¨ç­‰çŠ¶æ€

### é”™è¯¯å¤„ç†

1. **æ•è·å¼‚å¸¸**ï¼šå¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
2. **åº”ç”¨æƒ©ç½š**ï¼šé­é‡åçˆ¬æ—¶åº”ç”¨é€Ÿç‡æƒ©ç½š
3. **è®°å½•æ—¥å¿—**ï¼šè¯¦ç»†è®°å½•æ“ä½œæ—¥å¿—

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å­ç±»æœªå®ç° run() æ–¹æ³•**
   - ç¡®ä¿å­ç±»å®ç°äº† `run()` æ–¹æ³•
   - æ£€æŸ¥æ–¹æ³•ç­¾åæ˜¯å¦æ­£ç¡®

2. **å¤„ç†å™¨æœªåˆå§‹åŒ–**
   - ç¡®ä¿è°ƒç”¨äº† `_initialize_handlers()` æ–¹æ³•
   - æ£€æŸ¥å¤„ç†å™¨æ˜¯å¦ä¸º None

3. **Redis è¿æ¥å¤±è´¥**
   - æ£€æŸ¥ Redis é…ç½®æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤ Redis æœåŠ¡æ˜¯å¦è¿è¡Œ
   - éªŒè¯ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸

4. **è¿›åº¦æ¢å¤å¤±è´¥**
   - æ£€æŸ¥è¿›åº¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
   - éªŒè¯è¿›åº¦æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…
   - ç¡®è®¤é…ç½®æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ

### è°ƒè¯•æŠ€å·§

```python
# æ£€æŸ¥å¤„ç†å™¨çŠ¶æ€
print(f"URL Extractor: {self.url_extractor}")
print(f"Navigation Handler: {self.navigation_handler}")
print(f"Pagination Handler: {self.pagination_handler}")

# æ£€æŸ¥æ”¶é›†çŠ¶æ€
print(f"å·²æ”¶é›† URL: {len(self.collected_urls)}")
print(f"å½“å‰é¡µ: {self.pagination_handler.current_page_num}")
print(f"é™é€Ÿç­‰çº§: {self.rate_controller.current_level}")

# æ£€æŸ¥ Redis çŠ¶æ€
if self.redis_manager:
    print(f"Redis å·²è¿æ¥")
else:
    print(f"Redis æœªå¯ç”¨")
```

---

## ğŸ“š æ–¹æ³•å‚è€ƒ

### BaseCollector æ–¹æ³•

| æ–¹æ³• | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `run()` | æ—  | URLCollectorResult | è¿è¡Œæ”¶é›†æµç¨‹ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰ |
| `_initialize_handlers()` | æ—  | None | åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨ |
| `_load_previous_urls()` | æ—  | None | ä» Redis åŠ è½½å†å² URL |
| `_resume_to_target_page()` | target_page_num, jump_widget_xpath, pagination_xpath | int | ä½¿ç”¨ä¸‰é˜¶æ®µç­–ç•¥æ¢å¤åˆ°ç›®æ ‡é¡µ |
| `_collect_phase_with_xpath()` | æ—  | None | æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨å…¬å…± XPath |
| `_collect_phase_with_llm()` | æ—  | None | æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨ LLM |
| `_save_progress()` | æ—  | None | ä¿å­˜æ”¶é›†è¿›åº¦ |
| `_is_progress_compatible()` | progress | bool | æ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é… |
| `_extract_urls_with_xpath()` | æ—  | bool | ä½¿ç”¨ XPath æå–å½“å‰é¡µçš„ URL |
| `_collect_page_with_llm()` | max_scrolls, no_new_threshold | bool | ä½¿ç”¨ LLM æ”¶é›†å•é¡µçš„ URL |
| `_create_result()` | æ—  | URLCollectorResult | åˆ›å»ºæ”¶é›†ç»“æœ |

---

*æœ€åæ›´æ–°: 2026-01-08*

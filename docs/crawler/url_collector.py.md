# url_collector.py - è¯¦æƒ…é¡µ URL æ”¶é›†å™¨

url_collector.py æ¨¡å—æä¾›è¯¦æƒ…é¡µ URL æ”¶é›†åŠŸèƒ½ï¼Œé€šè¿‡æ¢ç´¢é˜¶æ®µåˆ†æé¡µé¢æ¨¡å¼ï¼Œç„¶åä½¿ç”¨å…¬å…±è„šæœ¬æ‰¹é‡æ”¶é›† URLã€‚

---

## ğŸ“ æ–‡ä»¶è·¯å¾„

```
src/autospider/crawler/url_collector.py
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸš€ æ ¸å¿ƒç±»
- `URLCollector` - è¯¦æƒ…é¡µ URL æ”¶é›†å™¨ä¸»ç±»

### ğŸ”§ ä¸»è¦æ–¹æ³•
- `run()` - è¿è¡Œ URL æ”¶é›†æµç¨‹
- `_explore_phase()` - æ¢ç´¢é˜¶æ®µï¼šè¿›å…¥å¤šä¸ªè¯¦æƒ…é¡µ
- `_collect_phase_with_xpath()` - æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨å…¬å…± xpath
- `_collect_phase_with_llm()` - æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨ LLM
- `_generate_crawler_script()` - ç”Ÿæˆçˆ¬è™«è„šæœ¬

### ğŸ” å†…éƒ¨æ–¹æ³•
- `_handle_current_is_detail()` - å¤„ç†å½“å‰é¡µé¢å°±æ˜¯è¯¦æƒ…é¡µ
- `_handle_select_detail_links()` - å¤„ç†é€‰æ‹©è¯¦æƒ…é“¾æ¥
- `_handle_click_to_enter()` - å¤„ç†ç‚¹å‡»è¿›å…¥è¯¦æƒ…é¡µ
- `_validate_mark_ids()` - éªŒè¯ mark_id ä¸æ–‡æœ¬çš„åŒ¹é…
- `_resume_to_target_page()` - æ–­ç‚¹æ¢å¤åˆ°ç›®æ ‡é¡µ

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### URLCollector

è¯¦æƒ…é¡µ URL æ”¶é›†å™¨ä¸»ç±»ï¼Œç»§æ‰¿è‡ª BaseCollectorï¼Œå¢åŠ æ¢ç´¢é˜¶æ®µåŠŸèƒ½ã€‚

```python
from autospider.crawler.url_collector import URLCollector

# åˆ›å»ºæ”¶é›†å™¨
collector = URLCollector(
    page=page,
    list_url="https://example.com/list",
    task_description="æ”¶é›†æ‰€æœ‰å•†å“è¯¦æƒ…é¡µé“¾æ¥",
    explore_count=3,
    max_nav_steps=10,
    output_dir="output"
)

# è¿è¡Œæ”¶é›†æµç¨‹
result = await collector.run()

print(f"æ”¶é›†åˆ° {len(result.collected_urls)} ä¸ª URL")
```

### æ”¶é›†æµç¨‹

URLCollector å®ç°ä¸‰é˜¶æ®µæ”¶é›†æµç¨‹ï¼š

**Phase 1: å¯¼èˆªåˆ°åˆ—è¡¨é¡µ**
```python
# å¯¼èˆªåˆ°åˆ—è¡¨é¡µ
await page.goto(list_url, wait_until="domcontentloaded", timeout=30000)
```

**Phase 2: å¯¼èˆªé˜¶æ®µï¼ˆç­›é€‰æ“ä½œï¼‰**
```python
# è®© LLM æ ¹æ®ä»»åŠ¡æè¿°è¿›è¡Œç­›é€‰æ“ä½œ
nav_success = await navigation_handler.run_navigation_phase()
```

**Phase 3: æ¢ç´¢é˜¶æ®µ**
```python
# è¿›å…¥ N ä¸ªä¸åŒçš„è¯¦æƒ…é¡µï¼Œè®°å½•æ“ä½œæ­¥éª¤
await collector._explore_phase()

# æå–å…¬å…± xpath
common_xpath = xpath_extractor.extract_common_xpath(detail_visits)
```

**Phase 4: æ”¶é›†é˜¶æ®µ**
```python
# ä½¿ç”¨å…¬å…± xpath éå†åˆ—è¡¨é¡µ
await collector._collect_phase_with_xpath()

# æˆ–ä½¿ç”¨ LLM éå†
await collector._collect_phase_with_llm()
```

**Phase 5: ç”Ÿæˆçˆ¬è™«è„šæœ¬**
```python
# ç”Ÿæˆ Scrapy + scrapy-playwright çˆ¬è™«è„šæœ¬
crawler_script = await collector._generate_crawler_script()
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### ä¸‰é˜¶æ®µæ”¶é›†æµç¨‹

1. **æ¢ç´¢é˜¶æ®µ**ï¼šè¿›å…¥ N ä¸ªä¸åŒçš„è¯¦æƒ…é¡µï¼Œè®°å½•æ¯æ¬¡è¿›å…¥çš„æ“ä½œæ­¥éª¤
2. **åˆ†æé˜¶æ®µ**ï¼šåˆ†æè¿™ N æ¬¡æ“ä½œçš„å…±åŒæ¨¡å¼ï¼Œæå–å…¬å…±è„šæœ¬
3. **æ”¶é›†é˜¶æ®µ**ï¼šä½¿ç”¨å…¬å…±è„šæœ¬éå†åˆ—è¡¨é¡µï¼Œæ”¶é›†æ‰€æœ‰è¯¦æƒ…é¡µçš„ URL

### æ–­ç‚¹ç»­çˆ¬

æ”¯æŒä»ä¸Šæ¬¡ä¸­æ–­çš„ä½ç½®ç»§ç»­æ”¶é›†ï¼š

```python
# è‡ªåŠ¨åŠ è½½å†å²è¿›åº¦
previous_progress = progress_persistence.load_progress()

# æ¢å¤é€Ÿç‡æ§åˆ¶å™¨çŠ¶æ€
rate_controller.current_level = previous_progress.backoff_level
rate_controller.consecutive_success_count = previous_progress.consecutive_success_pages

# è·³è½¬åˆ°ç›®æ ‡é¡µ
actual_page = await collector._resume_to_target_page(target_page_num)
```

### mark_id éªŒè¯

éªŒè¯ LLM è¿”å›çš„ mark_id ä¸æ–‡æœ¬æ˜¯å¦åŒ¹é…ï¼š

```python
if config.url_collector.validate_mark_id:
    mark_ids = collector._validate_mark_ids(mark_id_text_map, snapshot, screenshot_base64)
```

### é€Ÿç‡æ§åˆ¶

è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶ï¼Œé­é‡åçˆ¬æ—¶è‡ªåŠ¨é™é€Ÿï¼š

```python
# åº”ç”¨é€Ÿç‡æ§åˆ¶å»¶è¿Ÿ
delay = rate_controller.get_delay()
await asyncio.sleep(delay)

# è®°å½•æˆåŠŸ
rate_controller.record_success()

# åº”ç”¨æƒ©ç½šï¼ˆé­é‡åçˆ¬æ—¶ï¼‰
rate_controller.apply_penalty()
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„æ”¶é›†æµç¨‹

```python
import asyncio
from playwright.async_api import async_playwright
from autospider.crawler.url_collector import URLCollector

async def collect_urls():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        # åˆ›å»ºæ”¶é›†å™¨
        collector = URLCollector(
            page=page,
            list_url="https://example.com/products",
            task_description="æ”¶é›†æ‰€æœ‰å•†å“è¯¦æƒ…é¡µé“¾æ¥",
            explore_count=3,
            output_dir="output"
        )

        # è¿è¡Œæ”¶é›†æµç¨‹
        result = await collector.run()

        print(f"æ¢ç´¢äº† {len(result.detail_visits)} ä¸ªè¯¦æƒ…é¡µ")
        print(f"æ”¶é›†åˆ° {len(result.collected_urls)} ä¸ª URL")

        await browser.close()

# è¿è¡Œ
asyncio.run(collect_urls())
```

### è‡ªå®šä¹‰æ¢ç´¢æ•°é‡

```python
# æ¢ç´¢æ›´å¤šè¯¦æƒ…é¡µä»¥è·å¾—æ›´å‡†ç¡®çš„æ¨¡å¼
collector = URLCollector(
    page=page,
    list_url="https://example.com/list",
    task_description="æ”¶é›†æ–‡ç« è¯¦æƒ…é¡µé“¾æ¥",
    explore_count=5,  # æ¢ç´¢ 5 ä¸ªè¯¦æƒ…é¡µ
    max_nav_steps=15,  # æœ€å¤š 15 ä¸ªå¯¼èˆªæ­¥éª¤
    output_dir="output"
)
```

### æ–­ç‚¹ç»­çˆ¬

```python
# æ”¶é›†å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤ä¹‹å‰çš„è¿›åº¦
collector = URLCollector(
    page=page,
    list_url="https://example.com/list",
    task_description="æ”¶é›†å•†å“é“¾æ¥",
    output_dir="output"
)

# å¦‚æœä¹‹å‰ä¸­æ–­è¿‡ï¼Œä¼šè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­
result = await collector.run()
```

---

## ğŸ“ æœ€ä½³å®è·µ

### æ¢ç´¢é˜¶æ®µ

1. **åˆç†è®¾ç½®æ¢ç´¢æ•°é‡**ï¼šé€šå¸¸ 3-5 ä¸ªè¯¦æƒ…é¡µè¶³å¤Ÿæå–æ¨¡å¼
2. **ç¡®ä¿å¤šæ ·æ€§**ï¼šæ¢ç´¢ä¸åŒç±»å‹çš„è¯¦æƒ…é¡µ
3. **è®°å½•å¯¼èˆªæ­¥éª¤**ï¼šä¿å­˜ç­›é€‰æ“ä½œä»¥ä¾¿é‡æ”¾

### æ”¶é›†é˜¶æ®µ

1. **ä¼˜å…ˆä½¿ç”¨ XPath**ï¼šXPath æ”¶é›†æ¯” LLM æ”¶é›†æ›´å¿«é€Ÿã€æ›´ç¨³å®š
2. **è®¾ç½®åˆç†ç›®æ ‡**ï¼šæ ¹æ®å®é™…éœ€æ±‚è®¾ç½® target_url_count
3. **æ§åˆ¶ç¿»é¡µæ¬¡æ•°**ï¼šè®¾ç½® max_pages é¿å…æ— é™ç¿»é¡µ

### æ–­ç‚¹ç»­çˆ¬

1. **å®šæœŸä¿å­˜è¿›åº¦**ï¼šæ¯é¡µæ”¶é›†åä¿å­˜è¿›åº¦
2. **éªŒè¯é…ç½®åŒ¹é…**ï¼šç¡®ä¿å†å²é…ç½®ä¸å½“å‰ä»»åŠ¡åŒ¹é…
3. **æ¢å¤é€Ÿç‡çŠ¶æ€**ï¼šæ¢å¤é€Ÿç‡æ§åˆ¶å™¨çš„é™é€Ÿç­‰çº§

### é”™è¯¯å¤„ç†

1. **æ•è·å¼‚å¸¸**ï¼šå¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
2. **åº”ç”¨æƒ©ç½š**ï¼šé­é‡åçˆ¬æ—¶åº”ç”¨é€Ÿç‡æƒ©ç½š
3. **è®°å½•æ—¥å¿—**ï¼šè¯¦ç»†è®°å½•æ“ä½œæ—¥å¿—ä¾¿äºè°ƒè¯•

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¢ç´¢é˜¶æ®µå¤±è´¥**
   - æ£€æŸ¥åˆ—è¡¨é¡µ URL æ˜¯å¦æ­£ç¡®
   - éªŒè¯ä»»åŠ¡æè¿°æ˜¯å¦æ¸…æ™°
   - ç¡®è®¤é¡µé¢åŠ è½½å®Œæˆ

2. **XPath æå–å¤±è´¥**
   - æ£€æŸ¥æ¢ç´¢çš„è¯¦æƒ…é¡µæ•°é‡æ˜¯å¦è¶³å¤Ÿï¼ˆè‡³å°‘ 2 ä¸ªï¼‰
   - éªŒè¯è¯¦æƒ…é¡µ URL æ˜¯å¦æœ‰æ•ˆ
   - ç¡®è®¤å…ƒç´ é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®

3. **æ”¶é›†é˜¶æ®µå¡ä½**
   - æ£€æŸ¥åˆ†é¡µæ§ä»¶æ˜¯å¦æ­£ç¡®è¯†åˆ«
   - éªŒè¯é€Ÿç‡æ§åˆ¶å»¶è¿Ÿæ˜¯å¦åˆç†
   - ç¡®è®¤ç›®æ ‡ URL æ•°é‡æ˜¯å¦å¯è¾¾æˆ

4. **æ–­ç‚¹æ¢å¤å¤±è´¥**
   - æ£€æŸ¥å†å²é…ç½®æ˜¯å¦ä¸å½“å‰ä»»åŠ¡åŒ¹é…
   - éªŒè¯è¿›åº¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
   - ç¡®è®¤è·³è½¬æ§ä»¶ XPath æ˜¯å¦æ­£ç¡®

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥æ¢ç´¢è®°å½•
for visit in collector.detail_visits:
    print(f"è¯¦æƒ…é¡µ: {visit.detail_page_url}")
    print(f"ç‚¹å‡»å…ƒç´ : {visit.clicked_element_text}")
    print(f"XPath å€™é€‰: {visit.clicked_element_xpath_candidates}")

# æ£€æŸ¥æ”¶é›†è¿›åº¦
print(f"å½“å‰é¡µ: {pagination_handler.current_page_num}")
print(f"å·²æ”¶é›†: {len(collector.collected_urls)}")
print(f"é™é€Ÿç­‰çº§: {rate_controller.current_level}")
```

---

## ğŸ“š æ–¹æ³•å‚è€ƒ

### URLCollector æ–¹æ³•

| æ–¹æ³• | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `run()` | æ—  | URLCollectorResult | è¿è¡Œ URL æ”¶é›†æµç¨‹ |
| `_explore_phase()` | æ—  | None | æ¢ç´¢é˜¶æ®µï¼šè¿›å…¥å¤šä¸ªè¯¦æƒ…é¡µ |
| `_collect_phase_with_xpath()` | æ—  | None | æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨å…¬å…± xpath |
| `_collect_phase_with_llm()` | æ—  | None | æ”¶é›†é˜¶æ®µï¼šä½¿ç”¨ LLM |
| `_generate_crawler_script()` | æ—  | str | ç”Ÿæˆçˆ¬è™«è„šæœ¬ |
| `_handle_current_is_detail()` | explored: int | bool | å¤„ç†å½“å‰é¡µé¢å°±æ˜¯è¯¦æƒ…é¡µ |
| `_handle_select_detail_links()` | llm_decision, snapshot, screenshot_base64, explored | int | å¤„ç†é€‰æ‹©è¯¦æƒ…é“¾æ¥ |
| `_handle_click_to_enter()` | llm_decision, snapshot | bool | å¤„ç†ç‚¹å‡»è¿›å…¥è¯¦æƒ…é¡µ |
| `_validate_mark_ids()` | mark_id_text_map, snapshot, screenshot_base64 | list[int] | éªŒè¯ mark_id ä¸æ–‡æœ¬çš„åŒ¹é… |
| `_resume_to_target_page()` | target_page_num, jump_widget_xpath, pagination_xpath | int | æ–­ç‚¹æ¢å¤åˆ°ç›®æ ‡é¡µ |

### ä¾¿æ·å‡½æ•°

| å‡½æ•° | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `collect_detail_urls()` | page, list_url, task_description, explore_count, output_dir | URLCollectorResult | æ”¶é›†è¯¦æƒ…é¡µ URL çš„ä¾¿æ·å‡½æ•° |

---

*æœ€åæ›´æ–°: 2026-01-08*

# Extractor æ¨¡å—

Extractor æ¨¡å—æ˜¯ AutoSpider çš„æ™ºèƒ½è§„åˆ™å‘ç°å¼•æ“ï¼Œé€šè¿‡ LLM ç†è§£é¡µé¢è¯­ä¹‰ï¼Œè‡ªåŠ¨åˆ†æå’Œæå–å…³é”®ä¿¡æ¯ã€‚è¯¥æ¨¡å—æ”¯æŒ URL æ”¶é›†å’Œ XPath è„šæœ¬ç”Ÿæˆï¼Œèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€ä»»åŠ¡è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„çˆ¬è™«è„šæœ¬ã€‚

---

## æ¨¡å—ç»“æ„

```
extractor/
â”œâ”€â”€ __init__.py          # æ¨¡å—å…¥å£ï¼Œå¯¼å‡º ConfigGenerator å’Œ RuleGenerator
â”œâ”€â”€ config_generator.py  # é…ç½®ç”Ÿæˆå™¨ï¼Œç”Ÿæˆçˆ¬è™«é…ç½®å’Œè„šæœ¬
â””â”€â”€ collector/           # URL æ”¶é›†å™¨
    â”œâ”€â”€ __init__.py      # æ”¶é›†å™¨æ¨¡å—å¯¼å‡º
    â””â”€â”€ url_collector.py # URL æ”¶é›†å™¨å®ç°
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸ¯ é…ç½®ç”Ÿæˆå™¨ (config_generator.py)
- `ConfigGenerator` - é…ç½®ç”Ÿæˆå™¨ä¸»ç±»
- `generate()` - ç”Ÿæˆé…ç½®å’Œè„šæœ¬
- `_collect_urls()` - æ”¶é›†è¯¦æƒ…é¡µ URL
- `_generate_xpath_script()` - ç”Ÿæˆ XPath è„šæœ¬
- `_generate_config()` - ç”Ÿæˆé…ç½®æ–‡ä»¶

### ğŸ” URL æ”¶é›†å™¨ (url_collector.py)
- `URLCollector` - URL æ”¶é›†å™¨ä¸»ç±»
- `run()` - æ‰§è¡Œ URL æ”¶é›†ä»»åŠ¡
- `explore()` - æ¢ç´¢é˜¶æ®µï¼Œè®¿é—®è¯¦æƒ…é¡µæ ·æœ¬
- `collect()` - æ”¶é›†é˜¶æ®µï¼Œæ‰¹é‡æ”¶é›† URL
- `analyze()` - åˆ†æé˜¶æ®µï¼Œæå–å…¬å…± XPath æ¨¡å¼

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### ConfigGenerator

ConfigGenerator æ˜¯é…ç½®ç”Ÿæˆå™¨çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£å°†è‡ªç„¶è¯­è¨€ä»»åŠ¡è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„çˆ¬è™«é…ç½®å’Œè„šæœ¬ã€‚

```python
from autospider import ConfigGenerator

generator = ConfigGenerator()

result = await generator.generate(
    list_url="https://example.com/products",
    task_description="é‡‡é›†å•†å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬å•†å“åç§°ã€ä»·æ ¼ã€åº“å­˜çŠ¶æ€",
    max_pages=10
)

print(f"ç”Ÿæˆçš„é…ç½®: {result.config}")
print(f"ç”Ÿæˆçš„è„šæœ¬: {result.script}")
```

### URLCollector

URLCollector æ˜¯ URL æ”¶é›†å™¨çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£ä»åˆ—è¡¨é¡µæ”¶é›†è¯¦æƒ…é¡µ URLã€‚

```python
from autospider import URLCollector

collector = URLCollector(
    list_url="https://example.com/products",
    task_description="é‡‡é›†å•†å“è¯¦æƒ…é¡µ",
    explore_count=5,
    common_detail_xpath=None,
    redis_manager=None
)

result = await collector.run()
print(f"æ”¶é›†åˆ° {len(result.detail_urls)} ä¸ªè¯¦æƒ…é¡µ URL")
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### æ™ºèƒ½è§„åˆ™å‘ç°

é€šè¿‡ LLM ç†è§£é¡µé¢è¯­ä¹‰ï¼Œè‡ªåŠ¨å‘ç°æ•°æ®æå–è§„åˆ™ï¼š

```python
# è‡ªåŠ¨è¯†åˆ«å•†å“ä¿¡æ¯å­—æ®µ
fields = {
    "å•†å“åç§°": "h1.product-title",
    "ä»·æ ¼": "span.price",
    "åº“å­˜": "div.stock-status",
    "æè¿°": "div.description"
}

# ç”Ÿæˆç¨³å®šçš„ XPath é€‰æ‹©å™¨
xpath_script = generator._generate_xpath_script(fields)
```

### å¤šé˜¶æ®µæ¢ç´¢

URLCollector é‡‡ç”¨ä¸‰é˜¶æ®µæ¢ç´¢ç­–ç•¥ï¼š

1. **æ¢ç´¢é˜¶æ®µ**ï¼šè®¿é—®è¯¦æƒ…é¡µæ ·æœ¬ï¼Œäº†è§£é¡µé¢ç»“æ„
2. **æ”¶é›†é˜¶æ®µ**ï¼šæ‰¹é‡æ”¶é›†è¯¦æƒ…é¡µ URL
3. **åˆ†æé˜¶æ®µ**ï¼šæå–å…¬å…± XPath æ¨¡å¼

### æ–­ç‚¹ç»­ä¼ 

æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤ URL æ”¶é›†ä»»åŠ¡ï¼š

```python
# ä¿å­˜æ”¶é›†è¿›åº¦
await collector.save_progress(current_page, collected_urls)

# æ¢å¤æ”¶é›†è¿›åº¦
current_page, collected_urls = await collector.load_progress()
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„é…ç½®ç”Ÿæˆæµç¨‹

```python
import asyncio
from autospider import ConfigGenerator

async def generate_crawler_config():
    """ç”Ÿæˆçˆ¬è™«é…ç½®å’Œè„šæœ¬"""

    generator = ConfigGenerator()

    # ç”Ÿæˆé…ç½®å’Œè„šæœ¬
    result = await generator.generate(
        list_url="https://example.com/products",
        task_description="é‡‡é›†å•†å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬å•†å“åç§°ã€ä»·æ ¼ã€åº“å­˜çŠ¶æ€å’Œå•†å“æè¿°",
        max_pages=10
    )

    # ä¿å­˜é…ç½®æ–‡ä»¶
    with open("config.yaml", "w", encoding="utf-8") as f:
        f.write(result.config)

    # ä¿å­˜è„šæœ¬æ–‡ä»¶
    with open("crawler_script.py", "w", encoding="utf-8") as f:
        f.write(result.script)

    print("é…ç½®å’Œè„šæœ¬å·²ç”Ÿæˆ")
    print(f"é…ç½®æ–‡ä»¶: config.yaml")
    print(f"è„šæœ¬æ–‡ä»¶: crawler_script.py")

    return result

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(generate_crawler_config())
```

### URL æ”¶é›†æµç¨‹

```python
import asyncio
from autospider import URLCollector

async def collect_product_urls():
    """æ”¶é›†å•†å“è¯¦æƒ…é¡µ URL"""

    collector = URLCollector(
        list_url="https://example.com/products",
        task_description="é‡‡é›†å•†å“è¯¦æƒ…é¡µ",
        explore_count=5,
        common_detail_xpath=None,
        redis_manager=None
    )

    # è¿è¡Œæ”¶é›†ä»»åŠ¡
    result = await collector.run()

    print(f"æ”¶é›†å®Œæˆ!")
    print(f"è¯¦æƒ…é¡µ URL æ•°é‡: {len(result.detail_urls)}")
    print(f"å…¬å…± XPath æ¨¡å¼: {result.common_xpath}")

    # ä¿å­˜ URL åˆ—è¡¨
    with open("product_urls.txt", "w", encoding="utf-8") as f:
        for url in result.detail_urls:
            f.write(url + "\n")

    print("URL åˆ—è¡¨å·²ä¿å­˜åˆ° product_urls.txt")

    return result

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(collect_product_urls())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### ä»»åŠ¡æè¿°

1. **æ¸…æ™°æ˜ç¡®**ï¼šä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„ä»»åŠ¡æè¿°
2. **å­—æ®µåˆ—ä¸¾**ï¼šæ˜ç¡®åˆ—å‡ºéœ€è¦æå–çš„å­—æ®µ
3. **ç¤ºä¾‹è¯´æ˜**ï¼šæä¾›æœŸæœ›çš„è¾“å‡ºæ ¼å¼
4. **çº¦æŸæ¡ä»¶**ï¼šè¯´æ˜ä»»ä½•ç‰¹æ®Šè¦æ±‚æˆ–çº¦æŸ

### é…ç½®ä¼˜åŒ–

1. **æ¢ç´¢æ•°é‡**ï¼šæ ¹æ®ç½‘ç«™å¤æ‚åº¦è°ƒæ•´ explore_count
2. **æœ€å¤§é¡µæ•°**ï¼šåˆç†è®¾ç½® max_pages é¿å…è¿‡åº¦é‡‡é›†
3. **XPath ä¼˜å…ˆçº§**ï¼šæä¾›ç¨³å®šçš„ XPath é€‰æ‹©å™¨
4. **ç¼“å­˜ç­–ç•¥**ï¼šåˆ©ç”¨ Redis ç¼“å­˜æé«˜æ•ˆç‡

### é”™è¯¯å¤„ç†

1. **è¶…æ—¶è®¾ç½®**ï¼šä¸ºæ¯ä¸ªæ“ä½œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
2. **é‡è¯•æœºåˆ¶**ï¼šå®ç°å¤±è´¥é‡è¯•é€»è¾‘
3. **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†è®°å½•æ“ä½œæ—¥å¿—
4. **å¼‚å¸¸æ•è·**ï¼šå¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é…ç½®ç”Ÿæˆå¤±è´¥**
   - æ£€æŸ¥ä»»åŠ¡æè¿°æ˜¯å¦æ¸…æ™°
   - éªŒè¯ç›®æ ‡ URL æ˜¯å¦å¯è®¿é—®
   - ç¡®è®¤ LLM API é…ç½®æ­£ç¡®

2. **URL æ”¶é›†ä¸å®Œæ•´**
   - å¢åŠ  explore_count å‚æ•°
   - æ£€æŸ¥é¡µé¢åŠ è½½æ˜¯å¦å®Œæ•´
   - éªŒè¯ XPath é€‰æ‹©å™¨å‡†ç¡®æ€§

3. **è„šæœ¬æ‰§è¡Œé”™è¯¯**
   - æ£€æŸ¥ç”Ÿæˆçš„è„šæœ¬è¯­æ³•
   - éªŒè¯ XPath é€‰æ‹©å™¨æœ‰æ•ˆæ€§
   - ç¡®è®¤é¡µé¢ç»“æ„æœªå‘ç”Ÿå˜åŒ–

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥ç”Ÿæˆç»“æœ
print(f"é…ç½®å†…å®¹: {result.config}")
print(f"è„šæœ¬å†…å®¹: {result.script}")

# éªŒè¯ URL åˆ—è¡¨
for url in result.detail_urls[:10]:
    print(f"URL: {url}")

# æµ‹è¯• XPath é€‰æ‹©å™¨
test_xpath = "//div[@class='product-item']"
elements = await page.query_selector_all(test_xpath)
print(f"æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
```

---

*æœ€åæ›´æ–°: 2026-01-08*

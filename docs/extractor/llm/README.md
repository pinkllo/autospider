# LLM å­æ¨¡å—

LLM å­æ¨¡å—æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ Prompt æ¨¡æ¿æ¸²æŸ“ã€LLM è°ƒç”¨å’Œå“åº”è§£æã€‚è¯¥æ¨¡å—æ˜¯ AutoSpider æ™ºèƒ½å†³ç­–çš„æ ¸å¿ƒç»„ä»¶ã€‚

---

## ğŸ“ æ¨¡å—ç»“æ„

```
src/autospider/extractor/llm/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ llm_client.py           # LLM å®¢æˆ·ç«¯
â”œâ”€â”€ prompt_renderer.py      # Prompt æ¸²æŸ“å™¨
â””â”€â”€ response_parser.py      # å“åº”è§£æå™¨
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸ¤– LLM å®¢æˆ·ç«¯ (llm_client.py)
- `LLMClient` - LLM å®¢æˆ·ç«¯ä¸»ç±»
- `call()` - è°ƒç”¨ LLM API
- `call_with_vision()` - è°ƒç”¨æ”¯æŒè§†è§‰çš„ LLM
- `stream()` - æµå¼è°ƒç”¨ LLM

### ğŸ“ Prompt æ¸²æŸ“å™¨ (prompt_renderer.py)
- `PromptRenderer` - Prompt æ¸²æŸ“å™¨ä¸»ç±»
- `render()` - æ¸²æŸ“ Prompt æ¨¡æ¿
- `render_from_file()` - ä»æ–‡ä»¶æ¸²æŸ“ Prompt
- `render_from_template()` - ä»æ¨¡æ¿å­—ç¬¦ä¸²æ¸²æŸ“ Prompt

### ğŸ” å“åº”è§£æå™¨ (response_parser.py)
- `ResponseParser` - å“åº”è§£æå™¨ä¸»ç±»
- `parse_action()` - è§£æåŠ¨ä½œå“åº”
- `parse_xpath()` - è§£æ XPath å“åº”
- `parse_config()` - è§£æé…ç½®å“åº”
- `parse_url_list()` - è§£æ URL åˆ—è¡¨å“åº”

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### LLM å®¢æˆ·ç«¯

LLMClient æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„æ¥å£ï¼Œæ”¯æŒæ–‡æœ¬å’Œè§†è§‰è¾“å…¥ã€‚

```python
from autospider.extractor.llm import LLMClient

client = LLMClient(
    api_key="your-api-key",
    model="gpt-4-vision",
    temperature=0.1,
    max_tokens=4096
)

# è°ƒç”¨ LLM
response = await client.call(
    prompt="è¯·åˆ†æè¿™ä¸ªé¡µé¢çš„ç»“æ„",
    image_base64="iVBORw0KGgoAAAANS..."
)

print(f"LLM å“åº”: {response}")
```

### Prompt æ¸²æŸ“å™¨

PromptRenderer è´Ÿè´£æ¸²æŸ“ Prompt æ¨¡æ¿ï¼Œæ”¯æŒå˜é‡æ›¿æ¢å’Œæ¨¡æ¿ç»§æ‰¿ã€‚

```python
from autospider.extractor.llm import PromptRenderer

renderer = PromptRenderer()

# æ¸²æŸ“ Prompt
prompt = renderer.render(
    template="è¯·åˆ†æ{{task}}ï¼Œæå–{{fields}}å­—æ®µ",
    variables={
        "task": "å•†å“ä¿¡æ¯",
        "fields": "åç§°ã€ä»·æ ¼ã€åº“å­˜"
    }
)

print(f"æ¸²æŸ“åçš„ Prompt: {prompt}")
```

### å“åº”è§£æå™¨

ResponseParser è´Ÿè´£è§£æ LLM çš„å“åº”ï¼Œæå–ç»“æ„åŒ–æ•°æ®ã€‚

```python
from autospider.extractor.llm import ResponseParser

parser = ResponseParser()

# è§£æåŠ¨ä½œå“åº”
action = parser.parse_action(
    response='{"action": "click", "mark_id": 5, "thinking": "ç‚¹å‡»ç™»å½•æŒ‰é’®"}'
)

print(f"åŠ¨ä½œ: {action.action}")
print(f"æ ‡è®°ID: {action.mark_id}")
print(f"æ€è€ƒ: {action.thinking}")
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### å¤šæ¨¡æ€æ”¯æŒ

æ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œèƒ½å¤Ÿç†è§£é¡µé¢æˆªå›¾ï¼š

```python
# å¸¦å›¾åƒçš„ LLM è°ƒç”¨
response = await client.call_with_vision(
    prompt="è¯·è¯†åˆ«é¡µé¢ä¸­çš„ç™»å½•æŒ‰é’®",
    image_base64=screenshot_base64
)
```

### æ¨¡æ¿ç³»ç»Ÿ

æ”¯æŒ Jinja2 æ¨¡æ¿è¯­æ³•ï¼Œå®ç°å¤æ‚çš„ Prompt ç”Ÿæˆï¼š

```python
# ä½¿ç”¨ Jinja2 æ¨¡æ¿
template = """
ä»»åŠ¡: {{task}}
å­—æ®µ:
{% for field in fields %}
- {{field}}
{% endfor %}
"""

prompt = renderer.render(
    template=template,
    variables={
        "task": "é‡‡é›†å•†å“ä¿¡æ¯",
        "fields": ["åç§°", "ä»·æ ¼", "åº“å­˜"]
    }
)
```

### å“åº”éªŒè¯

è‡ªåŠ¨éªŒè¯ LLM å“åº”çš„æ ¼å¼å’Œå®Œæ•´æ€§ï¼š

```python
# éªŒè¯å“åº”æ ¼å¼
try:
    action = parser.parse_action(response)
    print(f"è§£ææˆåŠŸ: {action}")
except ValueError as e:
    print(f"è§£æå¤±è´¥: {e}")
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„ LLM äº¤äº’æµç¨‹

```python
import asyncio
from autospider.extractor.llm import LLMClient, PromptRenderer, ResponseParser

async def analyze_page_with_llm(page):
    """ä½¿ç”¨ LLM åˆ†æé¡µé¢"""

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = LLMClient(
        api_key="your-api-key",
        model="gpt-4-vision",
        temperature=0.1
    )

    # åˆ›å»ºæ¸²æŸ“å™¨
    renderer = PromptRenderer()

    # åˆ›å»ºè§£æå™¨
    parser = ResponseParser()

    # è·å–é¡µé¢æˆªå›¾
    screenshot = await page.screenshot(full_page=True)
    screenshot_base64 = base64.b64encode(screenshot).decode()

    # æ¸²æŸ“ Prompt
    prompt = renderer.render(
        template="è¯·åˆ†æé¡µé¢æˆªå›¾ï¼Œè¯†åˆ«{{element}}å…ƒç´ çš„ä½ç½®",
        variables={"element": "ç™»å½•æŒ‰é’®"}
    )

    # è°ƒç”¨ LLM
    response = await client.call_with_vision(
        prompt=prompt,
        image_base64=screenshot_base64
    )

    # è§£æå“åº”
    action = parser.parse_action(response)

    print(f"è¯†åˆ«åˆ°çš„åŠ¨ä½œ: {action.action}")
    print(f"æ ‡è®°ID: {action.mark_id}")

    return action

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(analyze_page_with_llm(page))
```

### æ‰¹é‡å¤„ç†

```python
import asyncio
from autospider.extractor.llm import LLMClient

async def batch_process(pages):
    """æ‰¹é‡å¤„ç†å¤šä¸ªé¡µé¢"""

    client = LLMClient(
        api_key="your-api-key",
        model="gpt-4-vision"
    )

    async def process_page(page):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        screenshot = await page.screenshot()
        screenshot_base64 = base64.b64encode(screenshot).decode()

        response = await client.call_with_vision(
            prompt="åˆ†æé¡µé¢ç»“æ„",
            image_base64=screenshot_base64
        )

        return response

    # å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢
    tasks = [process_page(page) for page in pages]
    results = await asyncio.gather(*tasks)

    return results

# ä½¿ç”¨ç¤ºä¾‹
results = asyncio.run(batch_process(pages))
```

---

## ğŸ“ æœ€ä½³å®è·µ

### Prompt è®¾è®¡

1. **æ¸…æ™°æ˜ç¡®**ï¼šä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„ Prompt
2. **ç»“æ„åŒ–è¾“å‡º**ï¼šè¦æ±‚ LLM è¾“å‡ºç»“æ„åŒ–æ•°æ®
3. **ç¤ºä¾‹å¼•å¯¼**ï¼šæä¾›ç¤ºä¾‹å¼•å¯¼ LLM ç†è§£
4. **çº¦æŸæ¡ä»¶**ï¼šæ˜ç¡®è¯´æ˜çº¦æŸæ¡ä»¶

### LLM è°ƒç”¨

1. **æ¸©åº¦è®¾ç½®**ï¼šæ ¹æ®ä»»åŠ¡è°ƒæ•´ temperature å‚æ•°
2. **Token é™åˆ¶**ï¼šåˆç†è®¾ç½® max_tokens å‚æ•°
3. **é‡è¯•æœºåˆ¶**ï¼šå®ç°å¤±è´¥é‡è¯•é€»è¾‘
4. **è¶…æ—¶æ§åˆ¶**ï¼šè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´

### å“åº”è§£æ

1. **æ ¼å¼éªŒè¯**ï¼šéªŒè¯å“åº”æ ¼å¼æ˜¯å¦æ­£ç¡®
2. **é”™è¯¯å¤„ç†**ï¼šå¦¥å–„å¤„ç†è§£æé”™è¯¯
3. **é»˜è®¤å€¼**ï¼šä¸ºå¯é€‰å­—æ®µæä¾›é»˜è®¤å€¼
4. **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†è®°å½•è§£æè¿‡ç¨‹

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **LLM è°ƒç”¨å¤±è´¥**
   - æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®
   - éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æœ‰æ•ˆ
   - ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸

2. **Prompt æ¸²æŸ“å¤±è´¥**
   - æ£€æŸ¥æ¨¡æ¿è¯­æ³•æ˜¯å¦æ­£ç¡®
   - éªŒè¯å˜é‡æ˜¯å¦å®Œæ•´
   - ç¡®è®¤æ¨¡æ¿æ–‡ä»¶è·¯å¾„æ­£ç¡®

3. **å“åº”è§£æå¤±è´¥**
   - æ£€æŸ¥å“åº”æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
   - éªŒè¯ JSON ç»“æ„æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤è§£æå™¨é…ç½®æ­£ç¡®

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥ LLM å“åº”
print(f"LLM å“åº”: {response}")
print(f"å“åº”é•¿åº¦: {len(response)}")

# æ£€æŸ¥ Prompt æ¸²æŸ“
print(f"æ¸²æŸ“åçš„ Prompt: {prompt}")
print(f"Prompt é•¿åº¦: {len(prompt)}")

# æ£€æŸ¥è§£æç»“æœ
print(f"è§£æç»“æœ: {action}")
print(f"è§£ææˆåŠŸ: {action is not None}")
```

---

*æœ€åæ›´æ–°: 2026-01-08*

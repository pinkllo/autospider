# Collector å­æ¨¡å—

Collector å­æ¨¡å—å®ç° URL æ”¶é›†åŠŸèƒ½ï¼Œè´Ÿè´£ä»åˆ—è¡¨é¡µè‡ªåŠ¨å‘ç°å¹¶æ”¶é›†è¯¦æƒ…é¡µ URLã€‚è¯¥æ¨¡å—é‡‡ç”¨å¤šé˜¶æ®µæ¢ç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿæ™ºèƒ½è¯†åˆ«è¯¦æƒ…é¡µé“¾æ¥å¹¶æå–å…¬å…± XPath æ¨¡å¼ã€‚

---

## ğŸ“ æ¨¡å—ç»“æ„

```
src/autospider/extractor/collector/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â””â”€â”€ url_collector.py         # URL æ”¶é›†å™¨å®ç°
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸ” URL æ”¶é›†å™¨ (url_collector.py)
- `URLCollector` - URL æ”¶é›†å™¨ä¸»ç±»
- `run()` - æ‰§è¡Œ URL æ”¶é›†ä»»åŠ¡
- `explore()` - æ¢ç´¢é˜¶æ®µï¼Œè®¿é—®è¯¦æƒ…é¡µæ ·æœ¬
- `collect()` - æ”¶é›†é˜¶æ®µï¼Œæ‰¹é‡æ”¶é›† URL
- `analyze()` - åˆ†æé˜¶æ®µï¼Œæå–å…¬å…± XPath æ¨¡å¼

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### URL æ”¶é›†å™¨

URLCollector æ˜¯ URL æ”¶é›†çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£ä»åˆ—è¡¨é¡µæ”¶é›†è¯¦æƒ…é¡µ URLã€‚

```python
from autospider.extractor.collector import URLCollector

collector = URLCollector(
    list_url="https://example.com/products",
    task_description="é‡‡é›†å•†å“è¯¦æƒ…é¡µ",
    explore_count=5,
    common_detail_xpath=None,
    redis_manager=None
)

result = await collector.run()
print(f"æ”¶é›†åˆ° {len(result.detail_urls)} ä¸ªè¯¦æƒ…é¡µ URL")
```

### å¤šé˜¶æ®µæ¢ç´¢

URLCollector é‡‡ç”¨ä¸‰é˜¶æ®µæ¢ç´¢ç­–ç•¥ï¼š

1. **æ¢ç´¢é˜¶æ®µ**ï¼šè®¿é—®è¯¦æƒ…é¡µæ ·æœ¬ï¼Œäº†è§£é¡µé¢ç»“æ„
2. **æ”¶é›†é˜¶æ®µ**ï¼šæ‰¹é‡æ”¶é›†è¯¦æƒ…é¡µ URL
3. **åˆ†æé˜¶æ®µ**ï¼šæå–å…¬å…± XPath æ¨¡å¼

```python
# æ‰§è¡Œæ¢ç´¢é˜¶æ®µ
await collector.explore()

# æ‰§è¡Œæ”¶é›†é˜¶æ®µ
await collector.collect()

# æ‰§è¡Œåˆ†æé˜¶æ®µ
await collector.analyze()
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### æ™ºèƒ½é“¾æ¥è¯†åˆ«

è‡ªåŠ¨è¯†åˆ«è¯¦æƒ…é¡µé“¾æ¥ï¼Œè¿‡æ»¤æ‰æ— å…³é“¾æ¥ï¼š

```python
# è¯†åˆ«è¯¦æƒ…é¡µé“¾æ¥
detail_links = [
    "https://example.com/product/123",
    "https://example.com/product/456",
    "https://example.com/product/789"
]

# è¿‡æ»¤æ‰æ— å…³é“¾æ¥
filtered_links = [
    link for link in all_links
    if "/product/" in link
]
```

### å…¬å…± XPath æå–

è‡ªåŠ¨æå–å…¬å…± XPath æ¨¡å¼ï¼Œç”¨äºæ‰¹é‡é‡‡é›†ï¼š

```python
# æå–å…¬å…± XPath
common_xpath = await collector.analyze()

print(f"å…¬å…± XPath: {common_xpath}")
```

### æ–­ç‚¹ç»­ä¼ 

æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤ URL æ”¶é›†ä»»åŠ¡ï¼š

```python
# ä¿å­˜æ”¶é›†è¿›åº¦
await collector.save_progress(current_page, collected_urls)

# æ¢å¤æ”¶é›†è¿›åº¦
current_page, collected_urls = await collector.load_progress()
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„ URL æ”¶é›†æµç¨‹

```python
import asyncio
from autospider.extractor.collector import URLCollector

async def collect_product_urls():
    """æ”¶é›†å•†å“è¯¦æƒ…é¡µ URL"""

    collector = URLCollector(
        list_url="https://example.com/products",
        task_description="é‡‡é›†å•†å“è¯¦æƒ…é¡µ",
        explore_count=5,
        common_detail_xpath=None,
        redis_manager=None
    )

    # è¿è¡Œæ”¶é›†ä»»åŠ¡
    result = await collector.run()

    print(f"æ”¶é›†å®Œæˆ!")
    print(f"è¯¦æƒ…é¡µ URL æ•°é‡: {len(result.detail_urls)}")
    print(f"å…¬å…± XPath æ¨¡å¼: {result.common_xpath}")

    # ä¿å­˜ URL åˆ—è¡¨
    with open("product_urls.txt", "w", encoding="utf-8") as f:
        for url in result.detail_urls:
            f.write(url + "\n")

    print("URL åˆ—è¡¨å·²ä¿å­˜åˆ° product_urls.txt")

    return result

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(collect_product_urls())
```

### è‡ªå®šä¹‰æ¢ç´¢ç­–ç•¥

```python
import asyncio
from autospider.extractor.collector import URLCollector

async def custom_collect():
    """è‡ªå®šä¹‰æ¢ç´¢ç­–ç•¥"""

    collector = URLCollector(
        list_url="https://example.com/products",
        task_description="é‡‡é›†å•†å“è¯¦æƒ…é¡µ",
        explore_count=10,  # å¢åŠ æ¢ç´¢æ•°é‡
        common_detail_xpath="//a[@class='product-link']",  # æä¾›å…¬å…± XPath
        redis_manager=None
    )

    # åˆ†é˜¶æ®µæ‰§è¡Œ
    await collector.explore()
    print(f"æ¢ç´¢å®Œæˆï¼Œå‘ç° {len(collector.detail_urls)} ä¸ªè¯¦æƒ…é¡µ")

    await collector.collect()
    print(f"æ”¶é›†å®Œæˆï¼Œå…± {len(collector.detail_urls)} ä¸ªè¯¦æƒ…é¡µ")

    await collector.analyze()
    print(f"åˆ†æå®Œæˆï¼Œå…¬å…± XPath: {collector.common_xpath}")

    return collector

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(custom_collect())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### æ¢ç´¢ç­–ç•¥

1. **æ¢ç´¢æ•°é‡**ï¼šæ ¹æ®ç½‘ç«™å¤æ‚åº¦è°ƒæ•´ explore_count
2. **å…¬å…± XPath**ï¼šæä¾›ç¨³å®šçš„å…¬å…± XPath æé«˜å‡†ç¡®æ€§
3. **è¿‡æ»¤è§„åˆ™**ï¼šä½¿ç”¨è¿‡æ»¤è§„åˆ™æ’é™¤æ— å…³é“¾æ¥
4. **å»é‡æœºåˆ¶**ï¼šç¡®ä¿ URL å”¯ä¸€æ€§

### æ€§èƒ½ä¼˜åŒ–

1. **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨æ‰¹é‡æ“ä½œæé«˜æ•ˆç‡
2. **å¹¶å‘æ§åˆ¶**ï¼šåˆç†æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
3. **ç¼“å­˜ç­–ç•¥**ï¼šåˆ©ç”¨ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚
4. **å»¶è¿Ÿæ§åˆ¶**ï¼šè®¾ç½®åˆç†çš„è¯·æ±‚å»¶è¿Ÿ

### é”™è¯¯å¤„ç†

1. **è¶…æ—¶è®¾ç½®**ï¼šä¸ºæ¯ä¸ªæ“ä½œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
2. **é‡è¯•æœºåˆ¶**ï¼šå®ç°å¤±è´¥é‡è¯•é€»è¾‘
3. **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†è®°å½•æ“ä½œæ—¥å¿—
4. **å¼‚å¸¸æ•è·**ï¼šå¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **URL æ”¶é›†ä¸å®Œæ•´**
   - å¢åŠ  explore_count å‚æ•°
   - æ£€æŸ¥é¡µé¢åŠ è½½æ˜¯å¦å®Œæ•´
   - éªŒè¯å…¬å…± XPath å‡†ç¡®æ€§

2. **å…¬å…± XPath æå–å¤±è´¥**
   - æ£€æŸ¥é¡µé¢ç»“æ„æ˜¯å¦ä¸€è‡´
   - éªŒè¯è¯¦æƒ…é¡µé“¾æ¥æ ¼å¼
   - è°ƒæ•´åˆ†æç®—æ³•å‚æ•°

3. **æ–­ç‚¹ç»­ä¼ å¤±è´¥**
   - æ£€æŸ¥å­˜å‚¨åç«¯æ˜¯å¦æ­£å¸¸
   - éªŒè¯è¿›åº¦æ•°æ®å®Œæ•´æ€§
   - ç¡®è®¤æ¢å¤é€»è¾‘æ­£ç¡®æ€§

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥æ”¶é›†ç»“æœ
print(f"è¯¦æƒ…é¡µ URL æ•°é‡: {len(result.detail_urls)}")
for url in result.detail_urls[:10]:
    print(f"URL: {url}")

# æ£€æŸ¥å…¬å…± XPath
print(f"å…¬å…± XPath: {result.common_xpath}")

# æµ‹è¯• XPath é€‰æ‹©å™¨
test_xpath = result.common_xpath
elements = await page.query_selector_all(test_xpath)
print(f"æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
```

---

*æœ€åæ›´æ–°: 2026-01-08*

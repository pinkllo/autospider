# script_generator.py - çˆ¬è™«è„šæœ¬ç”Ÿæˆå™¨

script_generator.py æ¨¡å—æä¾›çˆ¬è™«è„šæœ¬ç”ŸæˆåŠŸèƒ½ï¼Œä»æ¢ç´¢è®°å½•ä¸­åˆ†æå…±åŒæ¨¡å¼ï¼Œç”Ÿæˆ Scrapy + scrapy-playwright çˆ¬è™«è„šæœ¬ã€‚

---

## ğŸ“ æ–‡ä»¶è·¯å¾„

```
src/autospider/extractor/output/script_generator.py
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸš€ æ ¸å¿ƒç±»
- `ScriptGenerator` - çˆ¬è™«è„šæœ¬ç”Ÿæˆå™¨ä¸»ç±»

### ğŸ”§ ä¸»è¦æ–¹æ³•
- `generate_scrapy_playwright_script()` - ç”Ÿæˆ Scrapy + scrapy-playwright çˆ¬è™«è„šæœ¬

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### ScriptGenerator

çˆ¬è™«è„šæœ¬ç”Ÿæˆå™¨ï¼Œä»æ¢ç´¢è®°å½•ä¸­åˆ†æå…±åŒæ¨¡å¼ï¼Œç”Ÿæˆ Scrapy + scrapy-playwright çˆ¬è™«è„šæœ¬ã€‚

```python
from autospider.extractor.output.script_generator import ScriptGenerator

# åˆ›å»ºè„šæœ¬ç”Ÿæˆå™¨
generator = ScriptGenerator(output_dir="output")

# ç”Ÿæˆçˆ¬è™«è„šæœ¬
script = await generator.generate_scrapy_playwright_script(
    list_url="https://example.com/list",
    task_description="æ”¶é›†å•†å“è¯¦æƒ…é¡µé“¾æ¥",
    detail_visits=detail_visits,
    nav_steps=nav_steps,
    collected_urls=collected_urls,
    common_detail_xpath=common_detail_xpath
)

print(f"ç”Ÿæˆçš„è„šæœ¬:\n{script}")
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### LLM é©±åŠ¨çš„è„šæœ¬ç”Ÿæˆ

ä½¿ç”¨ LLM åˆ†ææ¢ç´¢è®°å½•å¹¶ç”Ÿæˆè„šæœ¬ï¼š

```python
# ä½¿ç”¨æ¨¡æ¿å¼•æ“åŠ è½½å’Œæ¸²æŸ“ prompt
system_prompt = render_template(
    PROMPT_TEMPLATE_PATH,
    section="system_prompt",
)

user_prompt = render_template(
    PROMPT_TEMPLATE_PATH,
    section="user_prompt",
    variables={
        "list_url": list_url,
        "task_description": task_description,
        "detail_visits": json.dumps(detail_visits, ensure_ascii=False),
        "nav_steps": json.dumps(nav_steps, ensure_ascii=False),
        "common_detail_xpath": common_detail_xpath or "æœªæå–",
    }
)

# è°ƒç”¨ LLM ç”Ÿæˆè„šæœ¬
response = await self.llm.ainvoke(messages)
script = response.content
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from autospider.extractor.output.script_generator import ScriptGenerator

async def generate_script():
    # åˆ›å»ºè„šæœ¬ç”Ÿæˆå™¨
    generator = ScriptGenerator(output_dir="output")

    # ç”Ÿæˆçˆ¬è™«è„šæœ¬
    script = await generator.generate_scrapy_playwright_script(
        list_url="https://example.com/list",
        task_description="æ”¶é›†å•†å“è¯¦æƒ…é¡µé“¾æ¥",
        detail_visits=detail_visits,
        nav_steps=nav_steps,
        collected_urls=collected_urls,
        common_detail_xpath=common_detail_xpath
    )

    # ä¿å­˜è„šæœ¬
    script_file = Path("output/spider.py")
    script_file.write_text(script, encoding="utf-8")

    print(f"è„šæœ¬å·²ä¿å­˜åˆ°: {script_file}")
    print(f"è¿è¡Œæ–¹å¼: scrapy runspider {script_file} -o output.json")

# è¿è¡Œ
asyncio.run(generate_script())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### è„šæœ¬ç”Ÿæˆ

1. **æä¾›è¯¦ç»†çš„æ¢ç´¢è®°å½•**ï¼šæä¾›è¯¦ç»†çš„æ¢ç´¢è®°å½•å¸®åŠ© LLM ç†è§£
2. **åŒ…å«å¯¼èˆªæ­¥éª¤**ï¼šåŒ…å«å¯¼èˆªæ­¥éª¤ä½¿è„šæœ¬æ›´å®Œæ•´
3. **éªŒè¯è„šæœ¬è´¨é‡**ï¼šéªŒè¯ç”Ÿæˆçš„è„šæœ¬æ˜¯å¦å¯ä»¥æ­£å¸¸è¿è¡Œ

### è„šæœ¬ä½¿ç”¨

1. **æµ‹è¯•è„šæœ¬**ï¼šåœ¨å®é™…è¿è¡Œå‰æµ‹è¯•è„šæœ¬
2. **ä¼˜åŒ–æ€§èƒ½**ï¼šæ ¹æ®å®é™…éœ€æ±‚ä¼˜åŒ–è„šæœ¬æ€§èƒ½
3. **å¤„ç†å¼‚å¸¸**ï¼šæ·»åŠ é€‚å½“çš„å¼‚å¸¸å¤„ç†

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è„šæœ¬ç”Ÿæˆå¤±è´¥**
   - æ£€æŸ¥æ¢ç´¢è®°å½•æ˜¯å¦å®Œæ•´
   - éªŒè¯å¯¼èˆªæ­¥éª¤æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤ LLM å“åº”æ˜¯å¦æœ‰æ•ˆ

2. **è„šæœ¬æ— æ³•è¿è¡Œ**
   - æ£€æŸ¥è„šæœ¬è¯­æ³•æ˜¯å¦æ­£ç¡®
   - éªŒè¯ä¾èµ–æ˜¯å¦å®‰è£…
   - ç¡®è®¤é…ç½®æ˜¯å¦æ­£ç¡®

---

## ğŸ“š æ–¹æ³•å‚è€ƒ

### ScriptGenerator æ–¹æ³•

| æ–¹æ³• | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `generate_scrapy_playwright_script()` | list_url, task_description, detail_visits, nav_steps, collected_urls, common_detail_xpath | str | ç”Ÿæˆ Scrapy + scrapy-playwright çˆ¬è™«è„šæœ¬ |

---

## ğŸ“„ è„šæœ¬ç¤ºä¾‹

### Scrapy + scrapy-playwright è„šæœ¬

```python
import scrapy
from scrapy_playwright.page import PageMethod

class ProductSpider(scrapy.Spider):
    name = 'products'
    start_urls = ['https://example.com/list']
    
    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                meta={
                    'playwright': True,
                    'playwright_page_methods': [
                        PageMethod('wait_for_selector', '//a[@class="product-link"]')
                    ]
                }
            )
    
    def parse(self, response):
        # æå–å•†å“é“¾æ¥
        product_links = response.xpath('//a[@class="product-link"]/@href').getall()
        
        for link in product_links:
            yield response.follow(link, callback=self.parse_product)
    
    def parse_product(self, response):
        # æå–å•†å“ä¿¡æ¯
        yield {
            'url': response.url,
            'title': response.xpath('//h1/text()').get(),
            'price': response.xpath('//span[@class="price"]/text()').get(),
        }
```

---

*æœ€åæ›´æ–°: 2026-01-08*

# 断点重续三阶段策略说明

## 概述
断点重续功能已完全集成到 `url_collector.py` 中，能够在爬虫中断后自动恢复到上次停止的页码，避免从第1页重新开始。

## 三阶段策略

### 第一阶段：URL 规律爆破
- **策略**：分析列表页 URL 是否包含 `page=xx` 参数，直接构造目标页 URL 并跳转
- **优点**：速度最快，直接定位
- **适用场景**：URL 中包含明确页码参数的网站（如 `?page=5`, `?p=10`）

### 第二阶段：控件直达
- **策略**：使用页码输入框 + 确定按钮进行跳转
- **依赖**：需要在 Phase 3.6.1 成功提取到跳转控件的 XPath
- **优点**：准确，适用于有跳转功能的网站
- **适用场景**：列表页提供"跳转到第X页"功能的网站

### 第三阶段：首项检测与回溯（兜底方案）
- **策略**：从第1页开始，只检测每页第一条数据是否已收集
  - 如果首条已存在 → 快速翻到下一页
  - 如果首条不存在 → 回退一页，确保完整性
- **优点**：通用性强，适用于所有有分页的网站
- **缺点**：速度较慢（需要逐页检测）

## 工作流程

### 1. 首次运行
```
Phase 0: 连接 Redis，加载历史 URL（如有）
Phase 1: 导航到列表页
Phase 2: 导航阶段（筛选操作）
Phase 3.5: 提取公共 XPath
Phase 3.6: 提取分页控件 XPath
Phase 3.6.1: 提取跳转控件 XPath
Phase 4: 开始收集 URL（从第1页）
```

### 2. 中断后恢复
```
Phase 0: 连接 Redis，加载历史 URL
Phase 0.5: 加载进度文件 progress.json
  ↓ 检测到 current_page_num = 15
  ↓ 恢复速率控制器状态
Phase 1: 导航到列表页
Phase 2: 导航阶段（重放导航步骤）
Phase 3.5: 提取公共 XPath
Phase 3.6: 提取分页控件 XPath
Phase 3.6.1: 提取跳转控件 XPath
Phase 3.7: 断点恢复 ← 新增阶段
  ↓ 初始化 ResumeCoordinator
  ↓ 按优先级尝试三个策略
  ↓ 策略1: URL 规律爆破 → 成功/失败
  ↓ 策略2: 控件直达 → 成功/失败
  ↓ 策略3: 首项检测回溯 → 确保能到达某个页
  ↓ 更新 pagination_handler.current_page_num = 实际页码
Phase 4: 继续收集 URL（从实际到达的页码开始）
```

## 持久化数据

### 1. progress.json
保存当前进度信息：
```json
{
  "status": "RUNNING",
  "current_page_num": 15,
  "collected_count": 450,
  "backoff_level": 1,
  "consecutive_success_pages": 3,
  "last_updated": "2026-01-07T15:00:00"
}
```

### 2. collection_config.json
保存提取到的配置（XPath等）：
```json
{
  "nav_steps": [...],
  "common_detail_xpath": "//a[@class='detail-link']",
  "pagination_xpath": "button:has-text('下一页')",
  "jump_widget_xpath": {
    "input": "input[class*='page-input']",
    "button": "button:has-text('确定')"
  },
  "list_url": "...",
  "task_description": "..."
}
```

### 3. urls.txt
保存已收集的所有 URL（每行一个）

### 4. Redis（可选）
保存已收集的 URL，支持逻辑删除

## 使用示例

### 正常运行
```bash
python -m autospider collect-urls --url "https://example.com/list"
# 输出：
# [断点恢复] 无历史进度，从第1页开始
# Phase 4: 收集阶段...
# [Pagination] ✓ 翻页成功，当前第 2 页
# ...
```

### 中断后恢复
```bash
# Ctrl+C 中断
# 再次运行同样的命令
python -m autospider collect-urls --url "https://example.com/list"

# 输出：
# [断点恢复] 检测到上次中断在第 15 页
# [断点恢复] 已收集 450 个 URL
# [Phase 3.7] 断点恢复：尝试跳转到第 15 页...
# [恢复协调器] 尝试策略 1/3: URL规律爆破
# [URL规律爆破] ✓ 成功跳转到第 15 页
# [Phase 3.7] ✓ 已定位到第 15 页，继续收集
# Phase 4: 收集阶段...（从第15页继续）
```

## 配置项

在 `config.py` 中可以配置相关参数：
```python
url_collector:
  max_pages: 100           # 最大翻页次数
  target_url_count: 1000   # 目标收集数量
  # ... 其他配置
```

## 注意事项

1. **导航步骤重放**：如果列表页需要筛选操作（Phase 2），断点恢复会自动重放这些操作
2. **速率控制恢复**：会恢复之前的降速等级，避免重复触发反爬
3. **配置缓存**：第一次运行提取的 XPath 会保存，下次运行可直接使用（减少 LLM 调用）
4. **通用性**：即使前两个策略失败，第三阶段兜底策略能确保程序不会卡死

## 故障处理

如果断点恢复出现问题，可以手动清除进度文件重新开始：
```bash
# 删除进度文件
rm output/progress.json

# 保留已收集的 URL（在 Redis 或 urls.txt 中）
# 重新运行会自动去重
```

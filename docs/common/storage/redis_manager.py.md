# redis_manager.py - Redis é˜Ÿåˆ—ç®¡ç†å™¨

`redis_manager.py` æ¨¡å—æä¾›åŸºäº Redis Stream çš„å¯é é˜Ÿåˆ—ç®¡ç†åŠŸèƒ½ã€‚å®ƒä¸“ä¸ºåˆ†å¸ƒå¼çˆ¬è™«è®¾è®¡ï¼Œç¡®ä¿ä»»åŠ¡åœ¨å¤šæœºå¹¶å‘ç¯å¢ƒä¸‹ä¸ä¸¢å¤±ã€ä¸é‡å¤ï¼Œå¹¶å…·å¤‡å®Œå–„çš„æ•…éšœæ¢å¤èƒ½åŠ›ã€‚

---

## ğŸ“ æ–‡ä»¶è·¯å¾„

```
src/autospider/common/storage/redis_manager.py
```

---

## ğŸ“‘ æ ¸å¿ƒç±»ï¼š`RedisQueueManager`

è¯¥ç±»é›†æˆäº†æ•°æ®æŒä¹…åŒ–ã€ä»»åŠ¡åˆ†å‘å’ŒçŠ¶æ€ç®¡ç†ã€‚

### å­˜å‚¨æ¶æ„ (Architecture)
ä¸ºäº†å…¼é¡¾æ€§èƒ½ä¸å¯é æ€§ï¼Œé‡‡ç”¨äº† **Hash + Stream** çš„æ··åˆç»“æ„ï¼š
1. **Data Hash (`{prefix}:data`)**: ä»¥ URL çš„ Hash ä¸º Key å­˜å‚¨åŸå§‹æ•°æ®å’Œå…ƒæŒ‡æ ‡ï¼ˆå¦‚åˆ›å»ºæ—¶é—´ã€é‡è¯•æ¬¡æ•°ã€é”™è¯¯è®°å½•ï¼‰ã€‚èµ·åˆ°â€œæ•°æ®å­—å…¸â€å’Œâ€œå»é‡å™¨â€çš„ä½œç”¨ã€‚
2. **Task Stream (`{prefix}:stream`)**: å­˜å‚¨æŒ‡å‘ Hash Key çš„ç´¢å¼•ï¼ˆ`data_id`ï¼‰ã€‚ç”¨äºå®ç°ä»»åŠ¡çš„åˆ†å‘ã€‚
3. **Consumer Group (`{prefix}:workers`)**: å…è®¸å¤šä¸ªè¿›ç¨‹ä»¥ç«äº‰æ–¹å¼è·å–ä»»åŠ¡ã€‚

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½ä¸ä»£ç ç¤ºä¾‹

### 1. ç”Ÿäº§è€…ï¼šæ¨å…¥ä»»åŠ¡
æ¨å…¥æ“ä½œæ˜¯åŸå­æ€§çš„ã€‚åªæœ‰å½“ URL åœ¨ Hash ä¸­ä¸å­˜åœ¨æ—¶ï¼Œæ‰ä¼šå°†å…¶åŠ å…¥ Stream é˜Ÿåˆ—ã€‚

```python
from autospider.common.storage.redis_manager import RedisQueueManager

manager = RedisQueueManager(host="127.0.0.1", key_prefix="news_spider")
await manager.connect()

# metadata ä¼šéšä»»åŠ¡ä¿å­˜ï¼Œæ–¹ä¾¿åç»­å¤„ç†
await manager.push_task(
    item="https://example.com/p/123", 
    metadata={"source": "index_page"}
)

# æ‰¹é‡æ¨å…¥ï¼ˆä½¿ç”¨ Pipeline ä¼˜åŒ–é€Ÿåº¦ï¼‰
await manager.push_tasks_batch(["url1", "url2"], metadata_list=[...])
```

### 2. æ¶ˆè´¹è€…ï¼šå¯é æ¶ˆè´¹ (ACK æœºåˆ¶)
ä»»åŠ¡è¢« fetch åï¼Œä¼šè¿›å…¥è¯¥æ¶ˆè´¹è€…çš„ **PEL (Pending Entries List)**ã€‚å¦‚æœæ¶ˆè´¹è€…å´©æºƒè€Œæ²¡æœ‰å‘é€ ACKï¼Œä»»åŠ¡å°†æ°¸è¿œåœç•™åœ¨ PEL ä¸­ã€‚

```python
# è·å–ä»»åŠ¡ (count=5 è¡¨ç¤ºæ‰¹é‡è·å–)
tasks = await manager.fetch_task(consumer_name="node_A", block_ms=2000, count=5)

for stream_id, data_id, data in tasks:
    success = do_work(data['url'])
    
    if success:
        # æ˜¾å¼ç¡®è®¤ï¼Œä»»åŠ¡ä» PEL å½»åº•ç§»é™¤
        await manager.ack_task(stream_id)
    else:
        # æ ‡è®°å¤±è´¥ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨ç´¯åŠ é‡è¯•æ¬¡æ•°
        await manager.fail_task(stream_id, data_id, error_msg="Timeout")
```

### 3. æ•…éšœæ¢å¤ (Failover)
å¦‚æœæŸä¸ªèŠ‚ç‚¹ï¼ˆå¦‚ `node_A`ï¼‰åœ¨å¤„ç†ä¸­é€”å®•æœºï¼Œå…¶ PEL ä¸­çš„ä»»åŠ¡å¯ä»¥é€šè¿‡ `recover_stale_tasks` è¢«å…¶ä»–æ­£å¸¸èŠ‚ç‚¹â€œæå›â€ã€‚

```python
# æå›é€»è¾‘ï¼šå¯»æ‰¾è¶…è¿‡ 300 ç§’æ²¡æœ‰ä»»ä½•æ´»åŠ¨çš„åœæ»ä»»åŠ¡å¹¶é‡æ–°åˆ†é…ç»™è‡ªå·±
recovered = await manager.recover_stale_tasks(
    consumer_name="node_B", 
    max_idle_ms=300000 
)
```

---

## ğŸ› ï¸ æ–¹æ³•å‚è€ƒ (API Reference)

| æ–¹æ³• | åŠŸèƒ½æè¿° |
|------|----------|
| `connect()` | åˆå§‹åŒ– Redis è¿æ¥ï¼Œå¹¶è‡ªåŠ¨åˆ›å»ºæ¶ˆè´¹è€…ç»„ã€‚ |
| `push_task(item, metadata)` | å†™å…¥æ•°æ®ã€‚è¿”å› `True` è¡¨ç¤ºæ–°ä»»åŠ¡ï¼Œ`False` è¡¨ç¤ºé‡å¤ã€‚ |
| `fetch_task(...)` | ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ã€‚æ”¯æŒé˜»å¡æ¨¡å¼å’Œæ‰¹é‡è·å–ã€‚ |
| `ack_task(stream_id)` | ç¡®è®¤ä»»åŠ¡å®Œæˆã€‚æ­¤æ­¥å¿…ä¸å¯å°‘ï¼Œå¦åˆ™ä¼šå¯¼è‡´å†…å­˜æ³„æ¼ï¼ˆPEL å †ç§¯ï¼‰ã€‚ |
| `fail_task(...)` | å¤„ç†å¤±è´¥ã€‚è‹¥é‡è¯•æ¬¡æ•°è¶…è¿‡ `max_retries`ï¼Œä»»åŠ¡ä¼šè¿›å…¥æ­»ä¿¡é˜Ÿåˆ—ã€‚ |
| `recover_stale_tasks(...)` | æ•…éšœè½¬ç§»æ ¸å¿ƒæ–¹æ³•ã€‚å®šæœŸè°ƒç”¨å¯ç¡®ä¿ä»»åŠ¡ä¸å› èŠ‚ç‚¹ç¦»çº¿è€Œä¸¢å¤±ã€‚ |
| `get_stats()` | è·å–å®æ—¶ç›‘æ§æ•°æ®ï¼šä»»åŠ¡æ€»æ•°ã€é˜Ÿåˆ—ç§¯å‹æ•°ã€å„æ¶ˆè´¹è€…çŠ¶æ€ã€‚ |

---

*æœ€åæ›´æ–°: 2026-01-28*

# Storage å­æ¨¡å—

Storage å­æ¨¡å—æä¾›æ•°æ®æŒä¹…åŒ–å­˜å‚¨åŠŸèƒ½ï¼ŒåŒ…æ‹¬ Redis æ•°æ®ç®¡ç†å’Œé€šç”¨æŒä¹…åŒ–æ¥å£ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œæ•°æ®ç¼“å­˜ã€‚

---

## ğŸ“ æ¨¡å—ç»“æ„

```
src/autospider/common/storage/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ persistence.py           # æŒä¹…åŒ–åŸºç±»
â””â”€â”€ redis_manager.py         # Redis ç®¡ç†å™¨
```

---

## ğŸ“‘ å‡½æ•°ç›®å½•

### ğŸ’¾ æŒä¹…åŒ–åŸºç±» (persistence.py)
- `PersistenceBase` - æŒä¹…åŒ–åŸºç±»
- `save_data(key, data)` - ä¿å­˜æ•°æ®
- `load_data(key)` - åŠ è½½æ•°æ®
- `delete_data(key)` - åˆ é™¤æ•°æ®
- `exists(key)` - æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨

### ğŸ”´ Redis ç®¡ç†å™¨ (redis_manager.py)
- `RedisManager` - Redis ç®¡ç†å™¨ä¸»ç±»
- `connect()` - è¿æ¥åˆ° Redis
- `disconnect()` - æ–­å¼€è¿æ¥
- `save_item(item, metadata)` - ä¿å­˜å•ä¸ªæ•°æ®é¡¹
- `save_items_batch(items, metadata_list)` - æ‰¹é‡ä¿å­˜æ•°æ®é¡¹
- `load_items()` - åŠ è½½æ‰€æœ‰æ•°æ®é¡¹
- `mark_as_deleted(item)` - æ ‡è®°ä¸ºé€»è¾‘åˆ é™¤
- `mark_as_deleted_batch(items)` - æ‰¹é‡æ ‡è®°åˆ é™¤
- `is_deleted(item)` - æ£€æŸ¥æ˜¯å¦å·²åˆ é™¤
- `get_active_items()` - è·å–æ´»è·ƒæ•°æ®é¡¹
- `get_metadata(item)` - è·å–å…ƒæ•°æ®
- `get_count()` - è·å–æ€»æ•°
- `get_active_count()` - è·å–æ´»è·ƒæ•°é‡

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### æŒä¹…åŒ–åŸºç±»

PersistenceBase ç±»å®šä¹‰äº†ç»Ÿä¸€çš„æŒä¹…åŒ–æ¥å£ï¼Œæ”¯æŒå¤šç§å­˜å‚¨åç«¯çš„å®ç°ã€‚

```python
from autospider.common.storage.persistence import PersistenceBase

# åˆ›å»ºæŒä¹…åŒ–å®ä¾‹ï¼ˆå…·ä½“å®ç°ç”±å­ç±»æä¾›ï¼‰
storage = PersistenceBase()

# ä¿å­˜æ•°æ®
await storage.save_data("task_progress", {
    "current_page": 5,
    "collected_urls": ["url1", "url2", "url3"],
    "last_updated": "2026-01-08T10:00:00Z"
})

# åŠ è½½æ•°æ®
data = await storage.load_data("task_progress")
if data:
    print(f"å½“å‰é¡µç : {data['current_page']}")
    print(f"å·²æ”¶é›†URLæ•°é‡: {len(data['collected_urls'])}")

# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
if await storage.exists("task_progress"):
    print("ä»»åŠ¡è¿›åº¦æ•°æ®å­˜åœ¨")

# åˆ é™¤æ•°æ®
await storage.delete_data("task_progress")
```

### Redis æ•°æ®ç®¡ç†

RedisManager ç±»æä¾› Redis æ•°æ®åº“çš„å®Œæ•´æ“ä½œæ¥å£ï¼Œæ”¯æŒè¿æ¥ç®¡ç†ã€æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢ã€‚

```python
from autospider.common.storage.redis_manager import RedisManager

# åˆ›å»º Redis ç®¡ç†å™¨
redis_manager = RedisManager(
    host="localhost",
    port=6379,
    password=None,
    db=0,
    key_prefix="autospider:"
)

# è¿æ¥åˆ° Redis
await redis_manager.connect()

try:
    # å­˜å‚¨ä»»åŠ¡è¿›åº¦
    await redis_manager.save_item("https://example.com/page1")

    # æ‰¹é‡å­˜å‚¨
    await redis_manager.save_items_batch([
        "https://example.com/page2",
        "https://example.com/page3"
    ])

    # åŠ è½½æ‰€æœ‰æ•°æ®é¡¹
    items = await redis_manager.load_items()
    print(f"å·²åŠ è½½ {len(items)} ä¸ªæ•°æ®é¡¹")

    # è·å–æ´»è·ƒæ•°æ®é¡¹
    active_items = await redis_manager.get_active_items()
    print(f"æ´»è·ƒæ•°æ®é¡¹: {len(active_items)}")

    # æ ‡è®°ä¸ºåˆ é™¤
    await redis_manager.mark_as_deleted("https://example.com/page1")

    # è·å–å…ƒæ•°æ®
    metadata = await redis_manager.get_metadata("https://example.com/page1")
    print(f"å…ƒæ•°æ®: {metadata}")

finally:
    # æ–­å¼€è¿æ¥
    await redis_manager.disconnect()
```

---

## ğŸ’¡ ç‰¹æ€§è¯´æ˜

### æ•°æ®åºåˆ—åŒ–

æ”¯æŒå¤šç§æ•°æ®æ ¼å¼çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼š

```python
# å­˜å‚¨ä¸åŒç±»å‹çš„æ•°æ®
await redis_manager.save_item("string_data", "ç®€å•å­—ç¬¦ä¸²")
await redis_manager.save_item("number_data", "42")
await redis_manager.save_item("list_data", [1, 2, 3, 4, 5])
await redis_manager.save_item("dict_data", {"key": "value", "number": 123})
await redis_manager.save_item("complex_data", {
    "nested": {"deep": "value"},
    "list": ["a", "b", "c"],
    "timestamp": "2026-01-08T10:00:00Z"
})

# è‡ªåŠ¨ååºåˆ—åŒ–
string_val = await redis_manager.load_items()  # è¿”å› set[str]
```

### é”®å‘½åç©ºé—´

ä½¿ç”¨é”®å‰ç¼€å®ç°å‘½åç©ºé—´éš”ç¦»ï¼Œé¿å…ä¸åŒä»»åŠ¡é—´çš„æ•°æ®å†²çªï¼š

```python
# ä¸åŒä»»åŠ¡çš„é”®å‘½åç©ºé—´
task1_manager = RedisManager(key_prefix="autospider:task1:")
task2_manager = RedisManager(key_prefix="autospider:task2:")

# å­˜å‚¨åˆ°ä¸åŒçš„å‘½åç©ºé—´
await task1_manager.save_item("progress", {"page": 1})
await task2_manager.save_item("progress", {"page": 5})

# è·å–å„è‡ªçš„æ•°æ®
task1_items = await task1_manager.load_items()
task2_items = await task2_manager.load_items()

print(f"Task 1: {len(task1_items)} ä¸ªæ•°æ®é¡¹")
print(f"Task 2: {len(task2_items)} ä¸ªæ•°æ®é¡¹")
```

### é€»è¾‘åˆ é™¤

æ”¯æŒé€»è¾‘åˆ é™¤åŠŸèƒ½ï¼Œé€‚ç”¨äºéœ€è¦ä¿ç•™åˆ é™¤è®°å½•çš„åœºæ™¯ï¼š

```python
# ä¿å­˜æ•°æ®é¡¹
await redis_manager.save_item("https://example.com/page1")

# æ ‡è®°ä¸ºé€»è¾‘åˆ é™¤ï¼ˆä¸çœŸæ­£åˆ é™¤æ•°æ®ï¼‰
await redis_manager.mark_as_deleted("https://example.com/page1")

# æ£€æŸ¥æ˜¯å¦å·²åˆ é™¤
is_deleted = await redis_manager.is_deleted("https://example.com/page1")
print(f"æ˜¯å¦å·²åˆ é™¤: {is_deleted}")

# è·å–æ´»è·ƒæ•°æ®é¡¹ï¼ˆä¸åŒ…æ‹¬å·²åˆ é™¤çš„ï¼‰
active_items = await redis_manager.get_active_items()
print(f"æ´»è·ƒæ•°æ®é¡¹: {len(active_items)}")

# è·å–æ‰€æœ‰æ•°æ®é¡¹ï¼ˆåŒ…æ‹¬å·²åˆ é™¤çš„ï¼‰
all_items = await redis_manager.load_items()
print(f"æ‰€æœ‰æ•°æ®é¡¹: {len(all_items)}")
```

### æ‰¹é‡æ“ä½œ

æ”¯æŒæ‰¹é‡æ“ä½œæé«˜æ€§èƒ½ï¼š

```python
# æ‰¹é‡ä¿å­˜
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3",
    "https://example.com/page4",
    "https://example.com/page5"
]

await redis_manager.save_items_batch(urls)
print(f"æ‰¹é‡ä¿å­˜äº† {len(urls)} ä¸ªæ•°æ®é¡¹")

# æ‰¹é‡æ ‡è®°åˆ é™¤
await redis_manager.mark_as_deleted_batch(urls[:3])
print(f"æ‰¹é‡æ ‡è®°åˆ é™¤äº† 3 ä¸ªæ•°æ®é¡¹")

# è·å–æ´»è·ƒæ•°é‡
active_count = await redis_manager.get_active_count()
print(f"æ´»è·ƒæ•°æ®é¡¹: {active_count}")
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### æ–­ç‚¹ç»­ä¼ å®ç°

```python
import asyncio
from autospider.common.storage.redis_manager import RedisManager

class ResumeManager:
    """æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""

    def __init__(self, redis_manager, task_id):
        self.redis = redis_manager
        self.task_id = task_id
        self.progress_key = f"task:{task_id}:progress"
        self.urls_key = f"task:{task_id}:urls"

    async def save_progress(self, current_page, collected_urls):
        """ä¿å­˜ä»»åŠ¡è¿›åº¦"""
        progress_data = {
            "current_page": current_page,
            "collected_urls": collected_urls,
            "last_saved": "2026-01-08T10:00:00Z",
            "total_collected": len(collected_urls)
        }

        # ä¿å­˜è¿›åº¦æ•°æ®
        await self.redis.save_item(self.progress_key, progress_data)

        # æ‰¹é‡ä¿å­˜ URL
        if collected_urls:
            await self.redis.save_items_batch(collected_urls)

        print(f"è¿›åº¦å·²ä¿å­˜: é¡µç  {current_page}, URLæ•°é‡ {len(collected_urls)}")

    async def load_progress(self):
        """åŠ è½½ä»»åŠ¡è¿›åº¦"""
        progress = await self.redis.get_metadata(self.progress_key)
        urls = await self.redis.get_active_items()

        if progress:
            print(f"ä»æ–­ç‚¹æ¢å¤: é¡µç  {progress.get('current_page', 1)}")
            return progress.get('current_page', 1), list(urls)
        else:
            print("æ— å†å²è¿›åº¦ï¼Œä»å¤´å¼€å§‹")
            return 1, []

    async def cleanup(self):
        """æ¸…ç†ä»»åŠ¡æ•°æ®"""
        await self.redis.mark_as_deleted(self.progress_key)
        print("ä»»åŠ¡æ•°æ®å·²æ¸…ç†")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    redis_manager = RedisManager(key_prefix="crawler:")
    await redis_manager.connect()

    try:
        resume_manager = ResumeManager(redis_manager, "task_123")

        # å°è¯•åŠ è½½å†å²è¿›åº¦
        start_page, existing_urls = await resume_manager.load_progress()

        # æ¨¡æ‹Ÿçˆ¬å–è¿‡ç¨‹
        current_page = start_page
        collected_urls = existing_urls.copy()

        while current_page <= 10:
            # æ¨¡æ‹Ÿæ”¶é›†URL
            new_urls = [f"https://example.com/product/{current_page * 10 + i}"
                       for i in range(10)]
            collected_urls.extend(new_urls)

            # æ¯é¡µä¿å­˜è¿›åº¦
            await resume_manager.save_progress(current_page, collected_urls)

            current_page += 1

            # æ¨¡æ‹Ÿæ„å¤–ä¸­æ–­
            if current_page == 5:
                print("æ¨¡æ‹Ÿæ„å¤–ä¸­æ–­...")
                break

        print(f"ä»»åŠ¡å®Œæˆï¼Œå…±æ”¶é›† {len(collected_urls)} ä¸ªURL")

    finally:
        # æ¸…ç†æ•°æ®
        await resume_manager.cleanup()
        await redis_manager.disconnect()

asyncio.run(main())
```

### æ•°æ®ç¼“å­˜ç³»ç»Ÿ

```python
import asyncio
import time
from autospider.common.storage.redis_manager import RedisManager

class CacheManager:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, redis_manager, default_ttl=300):
        self.redis = redis_manager
        self.default_ttl = default_ttl

    async def get_with_cache(self, key, fetch_func, ttl=None):
        """å¸¦ç¼“å­˜çš„è·å–æ•°æ®"""

        # å°è¯•ä»ç¼“å­˜è·å–
        metadata = await self.redis.get_metadata(key)
        if metadata and not await self.redis.is_deleted(key):
            print(f"ç¼“å­˜å‘½ä¸­: {key}")
            return metadata.get("data")

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè·å–å‡½æ•°
        print(f"ç¼“å­˜æœªå‘½ä¸­: {key}, é‡æ–°è·å–...")
        fresh_data = await fetch_func()

        # å­˜å‚¨åˆ°ç¼“å­˜
        cache_ttl = ttl if ttl is not None else self.default_ttl
        await self.redis.save_item(key, {"data": fresh_data, "timestamp": time.time()})

        return fresh_data

    async def invalidate_cache(self, key_pattern):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        # è¿™é‡Œéœ€è¦å®ç°æ¨¡å¼åŒ¹é…åˆ é™¤
        print(f"ç¼“å­˜å¤±æ•ˆ: {key_pattern}")

# ä½¿ç”¨ç¤ºä¾‹
async def fetch_user_data(user_id):
    """æ¨¡æ‹Ÿè·å–ç”¨æˆ·æ•°æ®ï¼ˆè€—æ—¶æ“ä½œï¼‰"""
    print(f"æ­£åœ¨è·å–ç”¨æˆ· {user_id} çš„æ•°æ®...")
    await asyncio.sleep(1)
    return {
        "user_id": user_id,
        "name": f"ç”¨æˆ·{user_id}",
        "email": f"user{user_id}@example.com",
        "fetched_at": time.time()
    }

async def main():
    redis_manager = RedisManager(key_prefix="cache:")
    await redis_manager.connect()

    cache_manager = CacheManager(redis_manager, default_ttl=60)

    # ç¬¬ä¸€æ¬¡è·å–ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    user1 = await cache_manager.get_with_cache(
        "user:123",
        lambda: fetch_user_data(123),
        ttl=300
    )
    print(f"ç”¨æˆ·æ•°æ®: {user1}")

    # ç¬¬äºŒæ¬¡è·å–ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    user1_cached = await cache_manager.get_with_cache(
        "user:123",
        lambda: fetch_user_data(123)
    )
    print(f"ç”¨æˆ·æ•°æ®ï¼ˆç¼“å­˜ï¼‰: {user1_cached}")

    await redis_manager.disconnect()

asyncio.run(main())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### æ•°æ®è®¾è®¡

1. **ç»“æ„åŒ–å­˜å‚¨**ï¼šä½¿ç”¨ JSON æ ¼å¼å­˜å‚¨å¤æ‚æ•°æ®
2. **é”®å‘½åè§„èŒƒ**ï¼šä½¿ç”¨æ¸…æ™°çš„å‘½åç©ºé—´å’Œé”®å‘½å
3. **æ•°æ®ç‰ˆæœ¬æ§åˆ¶**ï¼šä¸ºé‡è¦æ•°æ®æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
4. **å¤‡ä»½ç­–ç•¥**ï¼šå®šæœŸå¤‡ä»½å…³é”®æ•°æ®

### æ€§èƒ½ä¼˜åŒ–

1. **æ‰¹é‡æ“ä½œ**ï¼šä½¿ç”¨æ‰¹é‡æ“ä½œå‡å°‘ç½‘ç»œå¾€è¿”
2. **è¿æ¥å¤ç”¨**ï¼šé¿å…é¢‘ç¹åˆ›å»ºå’Œå…³é—­è¿æ¥
3. **æ•°æ®å‹ç¼©**ï¼šå¯¹å¤§æ–‡æœ¬æ•°æ®å¯ç”¨å‹ç¼©
4. **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´

### é”™è¯¯å¤„ç†

1. **è¿æ¥é‡è¯•**ï¼šå®ç°è¿æ¥å¤±è´¥æ—¶çš„è‡ªåŠ¨é‡è¯•
2. **æ•°æ®éªŒè¯**ï¼šå­˜å‚¨å‰éªŒè¯æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
3. **å¼‚å¸¸å¤„ç†**ï¼šå¦¥å–„å¤„ç†å„ç§å­˜å‚¨å¼‚å¸¸
4. **çŠ¶æ€ç›‘æ§**ï¼šç›‘æ§å­˜å‚¨ç³»ç»Ÿçš„å¥åº·çŠ¶æ€

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥å¤±è´¥**
   - æ£€æŸ¥ Redis æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
   - éªŒè¯è¿æ¥å‚æ•°æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤ç½‘ç»œè¿é€šæ€§

2. **æ•°æ®ä¸¢å¤±**
   - æ£€æŸ¥è¿‡æœŸæ—¶é—´è®¾ç½®
   - éªŒè¯æ•°æ®åºåˆ—åŒ–æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤å­˜å‚¨ç©ºé—´æ˜¯å¦å……è¶³

3. **æ€§èƒ½é—®é¢˜**
   - ä¼˜åŒ–æ•°æ®ç»“æ„å’ŒæŸ¥è¯¢æ¨¡å¼
   - æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
   - è€ƒè™‘æ•°æ®åˆ†ç‰‡å’Œé›†ç¾¤

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥è¿æ¥çŠ¶æ€
if redis_manager.client:
    print("Redis è¿æ¥æ­£å¸¸")
else:
    print("Redis è¿æ¥å¼‚å¸¸")

# ç›‘æ§æ€§èƒ½
import time
start_time = time.time()
await redis_manager.save_item("test", "data")
end_time = time.time()
print(f"å†™å…¥æ“ä½œè€—æ—¶: {end_time - start_time:.3f}ç§’")

# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
metadata = await redis_manager.get_metadata("important_data")
if metadata and "data" in metadata:
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
else:
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")
```

---

*æœ€åæ›´æ–°: 2026-01-08*
